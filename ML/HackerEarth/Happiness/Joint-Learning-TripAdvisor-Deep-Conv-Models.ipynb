{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1050 Ti (CNMeM is disabled, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "import enchant.checker as checker\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Embedding, LSTM, MaxPooling1D, Merge\n",
    "from keras.layers import Dropout, Activation, Dense, Flatten, BatchNormalization, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove punctuations\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', r'', text)\n",
    "    # Remove newlines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Convert everything to lowercase\n",
    "    return text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Usually stay near the airport, but this trip w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stayed at this Hilton for 2 nights. It was lik...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stayed there one night, December 16, on the wa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just stayed here last weekend and have alrea...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My mother who is 90 and I stayed one night on ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Rating\n",
       "0  Usually stay near the airport, but this trip w...      5\n",
       "1  Stayed at this Hilton for 2 nights. It was lik...      4\n",
       "2  Stayed there one night, December 16, on the wa...      4\n",
       "3  I just stayed here last weekend and have alrea...      5\n",
       "4  My mother who is 90 and I stayed one night on ...      5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripadvisor = pd.read_csv('tripadvisor_1245-m.csv')\n",
    "tripadvisor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724868"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tripadvisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripadvisor['Review'] = tripadvisor['Review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripadvisor['Rating'] = tripadvisor['Rating'].apply(lambda x: 0 if x in ('1', '2') else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usually stay near the airport but this trip we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stayed at this hilton for 2 nights it was like...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stayed there one night december 16 on the way ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i just stayed here last weekend and have alrea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my mother who is 90 and i stayed one night on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  usually stay near the airport but this trip we...       1\n",
       "1  stayed at this hilton for 2 nights it was like...       1\n",
       "2  stayed there one night december 16 on the way ...       1\n",
       "3  i just stayed here last weekend and have alrea...       1\n",
       "4  my mother who is 90 and i stayed one night on ...       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripadvisor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripadvisor = tripadvisor.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for our first visit to paris we decided to sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>though it has a terrific location and occupies...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my first time to paris and i stayed at the ren...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this hotel caters to american tourists on crui...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cest un hotel a oublier tres vite</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  for our first visit to paris we decided to sta...       1\n",
       "1  though it has a terrific location and occupies...       0\n",
       "2  my first time to paris and i stayed at the ren...       1\n",
       "3  this hotel caters to american tourists on crui...       0\n",
       "4                  cest un hotel a oublier tres vite       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripadvisor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500609\n",
      "224259\n"
     ]
    }
   ],
   "source": [
    "print(len(tripadvisor[tripadvisor['Rating'] == 1]))\n",
    "print(len(tripadvisor[tripadvisor['Rating'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Learning of TripAdvisor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 6000\n",
    "MAXLEN = 500\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_DIMS = 300\n",
    "FILTERS_1 = 250\n",
    "FILTERS_2 = 500\n",
    "HIDDEN_DIMS = 250\n",
    "EPOCHS = 40\n",
    "KERNEL_SIZE = 3\n",
    "PRETRAINING_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(tripadvisor['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "with open('tokenizer-1234-m.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = pickle.load(open('tokenizer-1234.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripadvisor_bow = tokenizer.texts_to_sequences(tripadvisor['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripadvisor_target = tripadvisor['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripadvisor_bow = sequence.pad_sequences(tripadvisor_bow, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(724868, 500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripadvisor_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tripadvisor_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, tripadvisor_target, \n",
    "                                                            random_state=42, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(717619, 500)\n",
      "(717619,)\n",
      "(7249, 500)\n",
      "(7249,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validate.shape)\n",
    "print(y_validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train['Description'] = train['Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Is_Response'] = train['Is_Response'].apply(lambda x: 1 if x == 'happy' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Browser_Used</th>\n",
       "      <th>Device_Used</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id10326</td>\n",
       "      <td>the room was kind of clean but had a very stro...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id10327</td>\n",
       "      <td>i stayed at the crown plaza april   april   th...</td>\n",
       "      <td>Internet Explorer</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id10328</td>\n",
       "      <td>i booked this hotel through hotwire at the low...</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id10329</td>\n",
       "      <td>stayed here with husband and sons on the way t...</td>\n",
       "      <td>InternetExplorer</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id10330</td>\n",
       "      <td>my girlfriends and i stayed here to celebrate ...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID                                        Description  \\\n",
       "0  id10326  the room was kind of clean but had a very stro...   \n",
       "1  id10327  i stayed at the crown plaza april   april   th...   \n",
       "2  id10328  i booked this hotel through hotwire at the low...   \n",
       "3  id10329  stayed here with husband and sons on the way t...   \n",
       "4  id10330  my girlfriends and i stayed here to celebrate ...   \n",
       "\n",
       "        Browser_Used Device_Used  Is_Response  \n",
       "0               Edge      Mobile            0  \n",
       "1  Internet Explorer      Mobile            0  \n",
       "2            Mozilla      Tablet            0  \n",
       "3   InternetExplorer     Desktop            1  \n",
       "4               Edge      Tablet            0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = tokenizer.texts_to_sequences(train['Description'])\n",
    "train_bow = sequence.pad_sequences(train_bow, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38932, 500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38932\n"
     ]
    }
   ],
   "source": [
    "train_y = train['Is_Response']\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35038, 500)\n",
      "(3894,)\n",
      "(3894, 500)\n",
      "(3894,)\n"
     ]
    }
   ],
   "source": [
    "train_hx, val_hx, train_hy, val_hy = train_test_split(train_bow, train_y, \n",
    "                                                      random_state=42, test_size=0.1)\n",
    "print(train_hx.shape)\n",
    "print(val_hy.shape)\n",
    "print(val_hx.shape)\n",
    "print(val_hy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## ConvNet with 2 Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 200)          1200000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500, 200)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 498, 250)          150250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 249, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 247, 500)          375500    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,851,251\n",
      "Trainable params: 1,851,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_FEATURES, EMBEDDING_DIMS, input_length=MAXLEN))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(HIDDEN_DIMS))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 358899 samples, validate on 39878 samples\n",
      "Epoch 1/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9443Epoch 00001: val_acc improved from -inf to 0.95830, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-01-0.958.h5py\n",
      "358899/358899 [==============================] - 806s 2ms/step - loss: 0.1417 - acc: 0.9443 - val_loss: 0.1120 - val_acc: 0.9583\n",
      "Epoch 2/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9640Epoch 00002: val_acc improved from 0.95830 to 0.96211, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-02-0.962.h5py\n",
      "358899/358899 [==============================] - 798s 2ms/step - loss: 0.0975 - acc: 0.9640 - val_loss: 0.1042 - val_acc: 0.9621\n",
      "Epoch 3/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9711Epoch 00003: val_acc improved from 0.96211 to 0.96226, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-03-0.962.h5py\n",
      "358899/358899 [==============================] - 800s 2ms/step - loss: 0.0786 - acc: 0.9711 - val_loss: 0.1120 - val_acc: 0.9623\n",
      "Epoch 4/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9764Epoch 00004: val_acc improved from 0.96226 to 0.96755, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-04-0.968.h5py\n",
      "358899/358899 [==============================] - 807s 2ms/step - loss: 0.0642 - acc: 0.9764 - val_loss: 0.1092 - val_acc: 0.9676\n",
      "Epoch 5/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9801Epoch 00005: val_acc did not improve\n",
      "358899/358899 [==============================] - 807s 2ms/step - loss: 0.0537 - acc: 0.9801 - val_loss: 0.1142 - val_acc: 0.9673\n",
      "Epoch 6/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9824Epoch 00006: val_acc did not improve\n",
      "358899/358899 [==============================] - 807s 2ms/step - loss: 0.0468 - acc: 0.9824 - val_loss: 0.1350 - val_acc: 0.9648\n",
      "Epoch 7/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9845Epoch 00007: val_acc improved from 0.96755 to 0.96818, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-07-0.968.h5py\n",
      "358899/358899 [==============================] - 807s 2ms/step - loss: 0.0407 - acc: 0.9845 - val_loss: 0.1152 - val_acc: 0.9682\n",
      "Epoch 8/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9859Epoch 00008: val_acc did not improve\n",
      "358899/358899 [==============================] - 806s 2ms/step - loss: 0.0379 - acc: 0.9859 - val_loss: 0.1254 - val_acc: 0.9674\n",
      "Epoch 9/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9867Epoch 00009: val_acc did not improve\n",
      "358899/358899 [==============================] - 799s 2ms/step - loss: 0.0356 - acc: 0.9867 - val_loss: 0.1616 - val_acc: 0.9619\n",
      "Epoch 10/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9873Epoch 00010: val_acc did not improve\n",
      "358899/358899 [==============================] - 793s 2ms/step - loss: 0.0335 - acc: 0.9873 - val_loss: 0.1463 - val_acc: 0.9681\n",
      "Epoch 11/100\n",
      "358848/358899 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9876Epoch 00011: val_acc did not improve\n",
      "358899/358899 [==============================] - 797s 2ms/step - loss: 0.0325 - acc: 0.9877 - val_loss: 0.1331 - val_acc: 0.9679\n",
      "Epoch 12/100\n",
      "  2176/358899 [..............................] - ETA: 12:52 - loss: 0.0219 - acc: 0.9913"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dd2d46049e8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, \n\u001b[0;32m----> 4\u001b[0;31m           validation_data=(x_validate, y_validate), callbacks=[checkpoint])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Monitor training accuracy and save the overtrained models\n",
    "fpath = 'data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-{epoch:02d}-{acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, monitor='acc', verbose=1, save_best_only=True)\n",
    "model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "          validation_data=(x_validate, y_validate), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save('data-aug-joint-2/data-aug-2-convs-tripadvisor-11-0.967.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Finetune all but the Embedding Layer of the Model (#1)\n",
    "The model trained on TripAdvisor dataset is fine-tuned. Use the model from the last iteration of the training procedure for TripAdvisor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.8925Epoch 00001: val_acc improved from -inf to 0.90036, saving model to data-aug-joint-2/finetuned-1-01-0.900.h5py\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.3044 - acc: 0.8925 - val_loss: 0.2618 - val_acc: 0.9004\n",
      "Epoch 2/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.9008Epoch 00002: val_acc improved from 0.90036 to 0.90190, saving model to data-aug-joint-2/finetuned-1-02-0.902.h5py\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.2615 - acc: 0.9009 - val_loss: 0.2534 - val_acc: 0.9019\n",
      "Epoch 3/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9050Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.2480 - acc: 0.9050 - val_loss: 0.2768 - val_acc: 0.8993\n",
      "Epoch 4/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9086Epoch 00004: val_acc improved from 0.90190 to 0.90241, saving model to data-aug-joint-2/finetuned-1-04-0.902.h5py\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.2372 - acc: 0.9086 - val_loss: 0.2583 - val_acc: 0.9024\n",
      "Epoch 5/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9133Epoch 00005: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.2269 - acc: 0.9133 - val_loss: 0.2586 - val_acc: 0.9009\n",
      "Epoch 6/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9177Epoch 00006: val_acc improved from 0.90241 to 0.90318, saving model to data-aug-joint-2/finetuned-1-06-0.903.h5py\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.2173 - acc: 0.9177 - val_loss: 0.2562 - val_acc: 0.9032\n",
      "Epoch 7/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9228Epoch 00007: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.2054 - acc: 0.9228 - val_loss: 0.2614 - val_acc: 0.8986\n",
      "Epoch 8/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9273Epoch 00008: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1929 - acc: 0.9273 - val_loss: 0.2727 - val_acc: 0.9001\n",
      "Epoch 9/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9338Epoch 00009: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1787 - acc: 0.9338 - val_loss: 0.2823 - val_acc: 0.9006\n",
      "Epoch 10/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9376Epoch 00010: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1652 - acc: 0.9376 - val_loss: 0.2937 - val_acc: 0.8991\n",
      "Epoch 11/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9436Epoch 00011: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1504 - acc: 0.9436 - val_loss: 0.3032 - val_acc: 0.8942\n",
      "Epoch 12/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9497Epoch 00012: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1350 - acc: 0.9497 - val_loss: 0.3396 - val_acc: 0.8942\n",
      "Epoch 13/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9564Epoch 00013: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1195 - acc: 0.9564 - val_loss: 0.3880 - val_acc: 0.8929\n",
      "Epoch 14/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9590Epoch 00014: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1098 - acc: 0.9590 - val_loss: 0.3850 - val_acc: 0.8921\n",
      "Epoch 15/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9660Epoch 00015: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0940 - acc: 0.9660 - val_loss: 0.4511 - val_acc: 0.8911\n",
      "Epoch 16/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9691Epoch 00016: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0839 - acc: 0.9691 - val_loss: 0.4396 - val_acc: 0.8891\n",
      "Epoch 17/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9720Epoch 00017: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0735 - acc: 0.9720 - val_loss: 0.4843 - val_acc: 0.8891\n",
      "Epoch 18/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9761Epoch 00018: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0639 - acc: 0.9761 - val_loss: 0.5207 - val_acc: 0.8911\n",
      "Epoch 19/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9785Epoch 00019: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0584 - acc: 0.9785 - val_loss: 0.5624 - val_acc: 0.8906\n",
      "Epoch 20/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9789Epoch 00020: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0536 - acc: 0.9789 - val_loss: 0.5679 - val_acc: 0.8885\n",
      "Epoch 21/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9824Epoch 00021: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0452 - acc: 0.9824 - val_loss: 0.5921 - val_acc: 0.8919\n",
      "Epoch 22/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9833Epoch 00022: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0424 - acc: 0.9833 - val_loss: 0.6113 - val_acc: 0.8906\n",
      "Epoch 23/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9854Epoch 00023: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0379 - acc: 0.9854 - val_loss: 0.6692 - val_acc: 0.8911\n",
      "Epoch 24/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9863Epoch 00024: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0355 - acc: 0.9863 - val_loss: 0.6790 - val_acc: 0.8937\n",
      "Epoch 25/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9878Epoch 00025: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0328 - acc: 0.9878 - val_loss: 0.6656 - val_acc: 0.8911\n",
      "Epoch 26/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9883Epoch 00026: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0313 - acc: 0.9883 - val_loss: 0.6961 - val_acc: 0.8919\n",
      "Epoch 27/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9888Epoch 00027: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0293 - acc: 0.9888 - val_loss: 0.7106 - val_acc: 0.8909\n",
      "Epoch 28/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9898Epoch 00028: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0277 - acc: 0.9898 - val_loss: 0.7217 - val_acc: 0.8903\n",
      "Epoch 29/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9893Epoch 00029: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0274 - acc: 0.9894 - val_loss: 0.7391 - val_acc: 0.8932\n",
      "Epoch 30/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9908Epoch 00030: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0239 - acc: 0.9908 - val_loss: 0.7423 - val_acc: 0.8947\n",
      "Epoch 31/100\n",
      "30912/35038 [=========================>....] - ETA: 7s - loss: 0.0242 - acc: 0.9914"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3b8be1b46f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, force)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mevt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-2/finetuned-1-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def first_model(model_weights=None):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_FEATURES, EMBEDDING_DIMS, input_length=MAXLEN))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(HIDDEN_DIMS))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    if model_weights is not None:\n",
    "        model.load_weights(model_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = first_model('data-aug-joint-2/finetuned-1-06-0.903.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test['Description'] = test['Description'].apply(clean_text)\n",
    "test_x = tokenizer.texts_to_sequences(test['Description'])\n",
    "test_x = sequence.pad_sequences(test_x, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29404/29404 [==============================] - 21s 699us/step\n"
     ]
    }
   ],
   "source": [
    "test_y = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID  0\n",
       "0  id80132  0\n",
       "1  id80133  1\n",
       "2  id80134  1\n",
       "3  id80135  0\n",
       "4  id80136  1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.concat([test['User_ID'], pd.Series(test_y.flatten())], axis=1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Is_Response\n",
       "0  id80132   not_happy\n",
       "1  id80133       happy\n",
       "2  id80134       happy\n",
       "3  id80135   not_happy\n",
       "4  id80136       happy"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.columns = ['User_ID', 'Is_Response']\n",
    "submission['Is_Response'] = submission['Is_Response'].apply(lambda x: 'happy' if x == 1 else 'not_happy')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission-joint-learning-2-convs-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Finetune all but the Embedding Layer of the Model (#2)\n",
    "The model trained on TripAdvisor dataset is fine-tuned. Use the model from the last iteration of the training procedure for TripAdvisor dataset. Use the best model obtained when trained on TripAdvisor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = first_model('data-aug-joint-2/data-aug-2-convs-tripadvisor-07-0.968.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.8853Epoch 00001: val_acc improved from -inf to 0.89676, saving model to data-aug-joint-2/finetuned-2-01-0.897.h5py\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.2987 - acc: 0.8854 - val_loss: 0.2681 - val_acc: 0.8968\n",
      "Epoch 2/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.8964Epoch 00002: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.2680 - acc: 0.8964 - val_loss: 0.2652 - val_acc: 0.8968\n",
      "Epoch 3/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9014Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.2521 - acc: 0.9014 - val_loss: 0.2697 - val_acc: 0.8909\n",
      "Epoch 4/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9066Epoch 00004: val_acc improved from 0.89676 to 0.89831, saving model to data-aug-joint-2/finetuned-2-04-0.898.h5py\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.2395 - acc: 0.9066 - val_loss: 0.2561 - val_acc: 0.8983\n",
      "Epoch 5/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9128Epoch 00005: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.2272 - acc: 0.9128 - val_loss: 0.2560 - val_acc: 0.8980\n",
      "Epoch 6/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9180Epoch 00006: val_acc improved from 0.89831 to 0.89959, saving model to data-aug-joint-2/finetuned-2-06-0.900.h5py\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.2129 - acc: 0.9180 - val_loss: 0.2639 - val_acc: 0.8996\n",
      "Epoch 7/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9232Epoch 00007: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.1996 - acc: 0.9232 - val_loss: 0.2701 - val_acc: 0.8980\n",
      "Epoch 8/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9308Epoch 00008: val_acc improved from 0.89959 to 0.90062, saving model to data-aug-joint-2/finetuned-2-08-0.901.h5py\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.1816 - acc: 0.9309 - val_loss: 0.2938 - val_acc: 0.9006\n",
      "Epoch 9/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9369Epoch 00009: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1668 - acc: 0.9369 - val_loss: 0.2975 - val_acc: 0.8937\n",
      "Epoch 10/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9461Epoch 00010: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1440 - acc: 0.9461 - val_loss: 0.3332 - val_acc: 0.8957\n",
      "Epoch 11/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9516Epoch 00011: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.1297 - acc: 0.9517 - val_loss: 0.3396 - val_acc: 0.8934\n",
      "Epoch 12/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9600Epoch 00012: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.1086 - acc: 0.9600 - val_loss: 0.4004 - val_acc: 0.8932\n",
      "Epoch 13/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9643Epoch 00013: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.0950 - acc: 0.9642 - val_loss: 0.4410 - val_acc: 0.8901\n",
      "Epoch 14/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9704Epoch 00014: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.0801 - acc: 0.9704 - val_loss: 0.4970 - val_acc: 0.8867\n",
      "Epoch 15/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9737Epoch 00015: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.0703 - acc: 0.9737 - val_loss: 0.4993 - val_acc: 0.8865\n",
      "Epoch 16/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9768Epoch 00016: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.0603 - acc: 0.9767 - val_loss: 0.5724 - val_acc: 0.8865\n",
      "Epoch 17/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9813Epoch 00017: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.0520 - acc: 0.9814 - val_loss: 0.6013 - val_acc: 0.8811\n",
      "Epoch 18/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9829Epoch 00018: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.0444 - acc: 0.9829 - val_loss: 0.6553 - val_acc: 0.8875\n",
      "Epoch 19/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9842Epoch 00019: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.0421 - acc: 0.9842 - val_loss: 0.6662 - val_acc: 0.8839\n",
      "Epoch 20/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9854Epoch 00020: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.0385 - acc: 0.9854 - val_loss: 0.6649 - val_acc: 0.8893\n",
      "Epoch 21/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9862Epoch 00021: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.0371 - acc: 0.9862 - val_loss: 0.6820 - val_acc: 0.8852\n",
      "Epoch 22/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9895Epoch 00022: val_acc did not improve\n",
      "35038/35038 [==============================] - 68s 2ms/step - loss: 0.0294 - acc: 0.9895 - val_loss: 0.7764 - val_acc: 0.8842\n",
      "Epoch 23/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9892Epoch 00023: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0303 - acc: 0.9892 - val_loss: 0.7461 - val_acc: 0.8826\n",
      "Epoch 24/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9889Epoch 00024: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0291 - acc: 0.9890 - val_loss: 0.7350 - val_acc: 0.8837\n",
      "Epoch 25/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9899Epoch 00025: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0280 - acc: 0.9899 - val_loss: 0.7546 - val_acc: 0.8832\n",
      "Epoch 26/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9907Epoch 00026: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0237 - acc: 0.9907 - val_loss: 0.7591 - val_acc: 0.8860\n",
      "Epoch 27/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9913Epoch 00027: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0252 - acc: 0.9913 - val_loss: 0.7710 - val_acc: 0.8844\n",
      "Epoch 28/100\n",
      "24128/35038 [===================>..........] - ETA: 19s - loss: 0.0199 - acc: 0.9920"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5b8caef61f28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-2/finetuned-2-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = first_model('data-aug-joint-2/finetuned-2-08-0.901.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29404/29404 [==============================] - 24s 802us/step\n"
     ]
    }
   ],
   "source": [
    "test_y = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID  0\n",
       "0  id80132  0\n",
       "1  id80133  1\n",
       "2  id80134  0\n",
       "3  id80135  0\n",
       "4  id80136  1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.concat([test['User_ID'], pd.Series(test_y.flatten())], axis=1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Is_Response\n",
       "0  id80132   not_happy\n",
       "1  id80133       happy\n",
       "2  id80134   not_happy\n",
       "3  id80135   not_happy\n",
       "4  id80136       happy"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.columns = ['User_ID', 'Is_Response']\n",
    "submission['Is_Response'] = submission['Is_Response'].apply(lambda x: 'happy' if x == 1 else 'not_happy')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission-joint-learning-2-convs-2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TripAdvisor Overtrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 200)          1200000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 200)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 498, 250)          150250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 249, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 247, 500)          375500    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,851,251\n",
      "Trainable params: 1,851,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_FEATURES, EMBEDDING_DIMS, input_length=MAXLEN))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(HIDDEN_DIMS))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 394789 samples, validate on 3988 samples\n",
      "Epoch 1/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9417Epoch 00001: acc improved from -inf to 0.94173, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-01-0.942.h5py\n",
      "394789/394789 [==============================] - 851s 2ms/step - loss: 0.1439 - acc: 0.9417 - val_loss: 0.0941 - val_acc: 0.9651\n",
      "Epoch 2/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9672Epoch 00002: acc improved from 0.94173 to 0.96715, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-02-0.967.h5py\n",
      "394789/394789 [==============================] - 847s 2ms/step - loss: 0.0890 - acc: 0.9672 - val_loss: 0.0871 - val_acc: 0.9697\n",
      "Epoch 3/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9756Epoch 00003: acc improved from 0.96715 to 0.97557, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-03-0.976.h5py\n",
      "394789/394789 [==============================] - 845s 2ms/step - loss: 0.0672 - acc: 0.9756 - val_loss: 0.0901 - val_acc: 0.9689\n",
      "Epoch 4/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9814Epoch 00004: acc improved from 0.97557 to 0.98142, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-04-0.981.h5py\n",
      "394789/394789 [==============================] - 847s 2ms/step - loss: 0.0506 - acc: 0.9814 - val_loss: 0.0913 - val_acc: 0.9709\n",
      "Epoch 5/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9853Epoch 00005: acc improved from 0.98142 to 0.98532, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-05-0.985.h5py\n",
      "394789/394789 [==============================] - 853s 2ms/step - loss: 0.0390 - acc: 0.9853 - val_loss: 0.0939 - val_acc: 0.9712\n",
      "Epoch 6/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9876Epoch 00006: acc improved from 0.98532 to 0.98756, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-06-0.988.h5py\n",
      "394789/394789 [==============================] - 849s 2ms/step - loss: 0.0321 - acc: 0.9876 - val_loss: 0.1090 - val_acc: 0.9694\n",
      "Epoch 7/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9891Epoch 00007: acc improved from 0.98756 to 0.98911, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-07-0.989.h5py\n",
      "394789/394789 [==============================] - 846s 2ms/step - loss: 0.0277 - acc: 0.9891 - val_loss: 0.1185 - val_acc: 0.9692\n",
      "Epoch 8/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9897Epoch 00008: acc improved from 0.98911 to 0.98970, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-08-0.990.h5py\n",
      "394789/394789 [==============================] - 839s 2ms/step - loss: 0.0254 - acc: 0.9897 - val_loss: 0.1198 - val_acc: 0.9687\n",
      "Epoch 9/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9908Epoch 00009: acc improved from 0.98970 to 0.99082, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-09-0.991.h5py\n",
      "394789/394789 [==============================] - 839s 2ms/step - loss: 0.0224 - acc: 0.9908 - val_loss: 0.1434 - val_acc: 0.9679\n",
      "Epoch 10/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9916Epoch 00010: acc improved from 0.99082 to 0.99163, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-10-0.992.h5py\n",
      "394789/394789 [==============================] - 839s 2ms/step - loss: 0.0206 - acc: 0.9916 - val_loss: 0.1338 - val_acc: 0.9707\n",
      "Epoch 11/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9919Epoch 00011: acc improved from 0.99163 to 0.99185, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-11-0.992.h5py\n",
      "394789/394789 [==============================] - 840s 2ms/step - loss: 0.0200 - acc: 0.9919 - val_loss: 0.1305 - val_acc: 0.9727\n",
      "Epoch 12/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9922Epoch 00012: acc improved from 0.99185 to 0.99223, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-12-0.992.h5py\n",
      "394789/394789 [==============================] - 848s 2ms/step - loss: 0.0185 - acc: 0.9922 - val_loss: 0.1478 - val_acc: 0.9727\n",
      "Epoch 13/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9926Epoch 00013: acc improved from 0.99223 to 0.99260, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-13-0.993.h5py\n",
      "394789/394789 [==============================] - 850s 2ms/step - loss: 0.0175 - acc: 0.9926 - val_loss: 0.1434 - val_acc: 0.9694\n",
      "Epoch 14/30\n",
      "394752/394789 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9930Epoch 00014: acc improved from 0.99260 to 0.99297, saving model to data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-14-0.993.h5py\n",
      "394789/394789 [==============================] - 841s 2ms/step - loss: 0.0167 - acc: 0.9930 - val_loss: 0.1453 - val_acc: 0.9707\n",
      "Epoch 15/30\n",
      "289152/394789 [====================>.........] - ETA: 3:44 - loss: 0.0154 - acc: 0.993 - ETA: 3:44 - loss: 0.0154 - acc: 0.9936"
     ]
    }
   ],
   "source": [
    "# Monitor training accuracy and save the overtrained models\n",
    "fpath = 'data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-{epoch:02d}-{acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, monitor='acc', verbose=1, save_best_only=True)\n",
    "model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=PRETRAINING_EPOCHS, \n",
    "          validation_data=(x_validate, y_validate), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Finetuning Overtrained Models (#4)\n",
    "Don't freeze any layer and finetune the TripAdvisor model on HackerEarth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def second_model(model_weights=None):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_FEATURES, EMBEDDING_DIMS, input_length=MAXLEN))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(HIDDEN_DIMS))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "                  metrics=['accuracy'])\n",
    "    if model_weights is not None:\n",
    "        model.load_weights(model_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = second_model('data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-14-0.993.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.8908Epoch 00001: val_acc improved from -inf to 0.89856, saving model to data-aug-joint-2/finetuned-4-01-0.899.h5py\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.3241 - acc: 0.8909 - val_loss: 0.2563 - val_acc: 0.8986\n",
      "Epoch 2/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9155Epoch 00002: val_acc improved from 0.89856 to 0.90010, saving model to data-aug-joint-2/finetuned-4-02-0.900.h5py\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.2212 - acc: 0.9155 - val_loss: 0.2535 - val_acc: 0.9001\n",
      "Epoch 3/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9363Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.1731 - acc: 0.9363 - val_loss: 0.2674 - val_acc: 0.8991\n",
      "Epoch 4/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9614Epoch 00004: val_acc did not improve\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.1123 - acc: 0.9614 - val_loss: 0.3303 - val_acc: 0.8937\n",
      "Epoch 5/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9811Epoch 00005: val_acc did not improve\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.0564 - acc: 0.9811 - val_loss: 0.4173 - val_acc: 0.8939\n",
      "Epoch 6/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9896Epoch 00006: val_acc did not improve\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.0306 - acc: 0.9896 - val_loss: 0.5117 - val_acc: 0.8970\n",
      "Epoch 7/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9937Epoch 00007: val_acc did not improve\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.0179 - acc: 0.9937 - val_loss: 0.6362 - val_acc: 0.8932\n",
      "Epoch 8/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9950Epoch 00008: val_acc improved from 0.90010 to 0.90036, saving model to data-aug-joint-2/finetuned-4-08-0.900.h5py\n",
      "35038/35038 [==============================] - 80s 2ms/step - loss: 0.0148 - acc: 0.9950 - val_loss: 0.6455 - val_acc: 0.9004\n",
      "Epoch 9/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9942Epoch 00009: val_acc did not improve\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.0162 - acc: 0.9942 - val_loss: 0.6559 - val_acc: 0.8901\n",
      "Epoch 10/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9945Epoch 00010: val_acc did not improve\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.0149 - acc: 0.9945 - val_loss: 0.6732 - val_acc: 0.8939\n",
      "Epoch 11/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9957Epoch 00011: val_acc did not improve\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.0120 - acc: 0.9957 - val_loss: 0.7006 - val_acc: 0.8909\n",
      "Epoch 12/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9956Epoch 00012: val_acc did not improve\n",
      "35038/35038 [==============================] - 79s 2ms/step - loss: 0.0128 - acc: 0.9956 - val_loss: 0.7405 - val_acc: 0.8929\n",
      "Epoch 13/100\n",
      "31936/35038 [==========================>...] - ETA: 6s - loss: 0.0100 - acc: 0.9963"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a9b8e57fdc77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-2/finetuned-4-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Finetuning Overtrained Models (#4)\n",
    "Freeze the Embedding layer and finetune the TripAdvisor model on HackerEarth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = second_model('data-aug-joint-1/data-aug-2-convs-tripadvisor-overtrained-13-0.993.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8885Epoch 00001: val_acc improved from -inf to 0.89343, saving model to data-aug-joint-2/finetuned-5-01-0.893.h5py\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.3235 - acc: 0.8885 - val_loss: 0.2694 - val_acc: 0.8934\n",
      "Epoch 2/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2522 - acc: 0.9015Epoch 00002: val_acc improved from 0.89343 to 0.89676, saving model to data-aug-joint-2/finetuned-5-02-0.897.h5py\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.2523 - acc: 0.9015 - val_loss: 0.2502 - val_acc: 0.8968\n",
      "Epoch 3/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9105Epoch 00003: val_acc improved from 0.89676 to 0.90087, saving model to data-aug-joint-2/finetuned-5-03-0.901.h5py\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.2307 - acc: 0.9105 - val_loss: 0.2523 - val_acc: 0.9009\n",
      "Epoch 4/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9202Epoch 00004: val_acc improved from 0.90087 to 0.90113, saving model to data-aug-joint-2/finetuned-5-04-0.901.h5py\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.2062 - acc: 0.9202 - val_loss: 0.2588 - val_acc: 0.9011\n",
      "Epoch 5/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9330Epoch 00005: val_acc did not improve\n",
      "35038/35038 [==============================] - 66s 2ms/step - loss: 0.1771 - acc: 0.9330 - val_loss: 0.2787 - val_acc: 0.8957\n",
      "Epoch 6/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9471Epoch 00006: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1411 - acc: 0.9471 - val_loss: 0.2993 - val_acc: 0.8965\n",
      "Epoch 7/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9610Epoch 00007: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.1054 - acc: 0.9610 - val_loss: 0.3520 - val_acc: 0.8919\n",
      "Epoch 8/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9740Epoch 00008: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0729 - acc: 0.9740 - val_loss: 0.3963 - val_acc: 0.8914\n",
      "Epoch 9/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9810Epoch 00009: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0511 - acc: 0.9810 - val_loss: 0.4741 - val_acc: 0.8896\n",
      "Epoch 10/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9861Epoch 00010: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0390 - acc: 0.9861 - val_loss: 0.5812 - val_acc: 0.8888\n",
      "Epoch 11/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9889Epoch 00011: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0297 - acc: 0.9888 - val_loss: 0.5724 - val_acc: 0.8867\n",
      "Epoch 12/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9903Epoch 00012: val_acc did not improve\n",
      "35038/35038 [==============================] - 67s 2ms/step - loss: 0.0264 - acc: 0.9903 - val_loss: 0.6037 - val_acc: 0.8873\n",
      "Epoch 13/100\n",
      " 2752/35038 [=>............................] - ETA: 58s - loss: 0.0269 - acc: 0.9916"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ec7b18b2d4f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-2/finetuned-5-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = second_model('data-aug-joint-2/finetuned-5-04-0.901.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29404/29404 [==============================] - 20s 666us/step\n"
     ]
    }
   ],
   "source": [
    "test_y = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID  0\n",
       "0  id80132  0\n",
       "1  id80133  1\n",
       "2  id80134  0\n",
       "3  id80135  0\n",
       "4  id80136  1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.concat([test['User_ID'], pd.Series(test_y.flatten())], axis=1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Is_Response\n",
       "0  id80132   not_happy\n",
       "1  id80133       happy\n",
       "2  id80134   not_happy\n",
       "3  id80135   not_happy\n",
       "4  id80136       happy"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.columns = ['User_ID', 'Is_Response']\n",
    "submission['Is_Response'] = submission['Is_Response'].apply(lambda x: 'happy' if x == 1 else 'not_happy')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission-joint-learning-data-augmentation-overtrained-fintuned-5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Convolutional Model With 4 Convolution Layers and 300-d Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def conv_4_e300(model_weights=None):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_FEATURES, EMBEDDING_DIMS, input_length=MAXLEN))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(HIDDEN_DIMS, activation='relu') )\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(HIDDEN_DIMS))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "                  metrics=['accuracy'])\n",
    "    if model_weights is not None:\n",
    "        model.load_weights(model_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = conv_4_e300()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 300)          1800000   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 498, 250)          225250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 249, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 247, 250)          187750    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 123, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 121, 500)          375500    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 60, 500)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 58, 500)           750500    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 29, 500)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 14500)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               3625250   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 7,027,251\n",
      "Trainable params: 7,027,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 717619 samples, validate on 7249 samples\n",
      "Epoch 1/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9467Epoch 00001: acc improved from -inf to 0.94666, saving model to data-aug-joint-3/data-aug-4-convs-tripadvisor-01-0.947.h5py\n",
      "717619/717619 [==============================] - 2355s 3ms/step - loss: 0.1394 - acc: 0.9467 - val_loss: 0.1059 - val_acc: 0.9625\n",
      "Epoch 2/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9661Epoch 00002: acc improved from 0.94666 to 0.96607, saving model to data-aug-joint-3/data-aug-4-convs-tripadvisor-02-0.966.h5py\n",
      "717619/717619 [==============================] - 2373s 3ms/step - loss: 0.0952 - acc: 0.9661 - val_loss: 0.0879 - val_acc: 0.9680\n",
      "Epoch 3/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9727Epoch 00003: acc improved from 0.96607 to 0.97268, saving model to data-aug-joint-3/data-aug-4-convs-tripadvisor-03-0.973.h5py\n",
      "717619/717619 [==============================] - 2381s 3ms/step - loss: 0.0775 - acc: 0.9727 - val_loss: 0.0850 - val_acc: 0.9699\n",
      "Epoch 4/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9783Epoch 00004: acc improved from 0.97268 to 0.97834, saving model to data-aug-joint-3/data-aug-4-convs-tripadvisor-04-0.978.h5py\n",
      "717619/717619 [==============================] - 2352s 3ms/step - loss: 0.0618 - acc: 0.9783 - val_loss: 0.0835 - val_acc: 0.9709\n",
      "Epoch 5/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9824Epoch 00005: acc improved from 0.97834 to 0.98237, saving model to data-aug-joint-3/data-aug-4-convs-tripadvisor-05-0.982.h5py\n",
      "717619/717619 [==============================] - 2333s 3ms/step - loss: 0.0497 - acc: 0.9824 - val_loss: 0.0989 - val_acc: 0.9698\n",
      "Epoch 6/40\n",
      "  6912/717619 [..............................] - ETA: 38:52 - loss: 0.0297 - acc: 0.9893"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-299c8102f002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(x_train, y_train, validation_data=(x_validate, y_validate), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-3/data-aug-4-convs-tripadvisor-{epoch:02d}-{acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='acc', save_best_only=True)\n",
    "model.fit(x_train, y_train, validation_data=(x_validate, y_validate), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save('data-aug-joint-3/data-aug-4-convs-tripadvisor-06-interrupted.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train['Description'] = train['Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_bow = tokenizer.texts_to_sequences(train['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train['Is_Response'] = train['Is_Response'].apply(lambda x: 1 if x == 'happy' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Browser_Used</th>\n",
       "      <th>Device_Used</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id10326</td>\n",
       "      <td>the room was kind of clean but had a very stro...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id10327</td>\n",
       "      <td>i stayed at the crown plaza april   april   th...</td>\n",
       "      <td>Internet Explorer</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id10328</td>\n",
       "      <td>i booked this hotel through hotwire at the low...</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id10329</td>\n",
       "      <td>stayed here with husband and sons on the way t...</td>\n",
       "      <td>InternetExplorer</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id10330</td>\n",
       "      <td>my girlfriends and i stayed here to celebrate ...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID                                        Description  \\\n",
       "0  id10326  the room was kind of clean but had a very stro...   \n",
       "1  id10327  i stayed at the crown plaza april   april   th...   \n",
       "2  id10328  i booked this hotel through hotwire at the low...   \n",
       "3  id10329  stayed here with husband and sons on the way t...   \n",
       "4  id10330  my girlfriends and i stayed here to celebrate ...   \n",
       "\n",
       "        Browser_Used Device_Used  Is_Response  \n",
       "0               Edge      Mobile            0  \n",
       "1  Internet Explorer      Mobile            0  \n",
       "2            Mozilla      Tablet            0  \n",
       "3   InternetExplorer     Desktop            1  \n",
       "4               Edge      Tablet            0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_y = train['Is_Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_bow = sequence.pad_sequences(train_bow, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38932, 500)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35038, 500)\n",
      "(3894,)\n",
      "(3894, 500)\n",
      "(3894,)\n"
     ]
    }
   ],
   "source": [
    "train_hx, val_hx, train_hy, val_hy = train_test_split(train_bow, train_y, \n",
    "                                                      random_state=42, test_size=0.1)\n",
    "print(train_hx.shape)\n",
    "print(val_hy.shape)\n",
    "print(val_hx.shape)\n",
    "print(val_hy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9007Epoch 00001: val_acc improved from -inf to 0.90601, saving model to data-aug-joint-3/finetuned-1-01-0.906.h5py\n",
      "35038/35038 [==============================] - 121s 3ms/step - loss: 0.2646 - acc: 0.9008 - val_loss: 0.2351 - val_acc: 0.9060\n",
      "Epoch 2/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9247Epoch 00002: val_acc improved from 0.90601 to 0.90627, saving model to data-aug-joint-3/finetuned-1-02-0.906.h5py\n",
      "35038/35038 [==============================] - 121s 3ms/step - loss: 0.1989 - acc: 0.9247 - val_loss: 0.2478 - val_acc: 0.9063\n",
      "Epoch 3/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9475Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 120s 3ms/step - loss: 0.1452 - acc: 0.9475 - val_loss: 0.2819 - val_acc: 0.8986\n",
      "Epoch 4/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9710Epoch 00004: val_acc did not improve\n",
      "35038/35038 [==============================] - 120s 3ms/step - loss: 0.0833 - acc: 0.9710 - val_loss: 0.3757 - val_acc: 0.8978\n",
      "Epoch 5/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9846Epoch 00005: val_acc did not improve\n",
      "35038/35038 [==============================] - 120s 3ms/step - loss: 0.0468 - acc: 0.9846 - val_loss: 0.4336 - val_acc: 0.8945\n",
      "Epoch 6/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9907Epoch 00006: val_acc did not improve\n",
      "35038/35038 [==============================] - 120s 3ms/step - loss: 0.0273 - acc: 0.9908 - val_loss: 0.5419 - val_acc: 0.8901\n",
      "Epoch 7/40\n",
      " 7936/35038 [=====>........................] - ETA: 1:28 - loss: 0.0184 - acc: 0.9937"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-67bfc0726c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-3/finetuned-1-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = conv_4_e300('data-aug-joint-3/finetuned-1-02-0.906.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test['Description'] = test['Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_x = tokenizer.texts_to_sequences(test['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_x = sequence.pad_sequences(test_x, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29404/29404 [==============================] - 31s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_y = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID  0\n",
       "0  id80132  0\n",
       "1  id80133  1\n",
       "2  id80134  1\n",
       "3  id80135  0\n",
       "4  id80136  1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.concat([test['User_ID'], pd.Series(test_y.flatten())], axis=1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Is_Response\n",
       "0  id80132   not_happy\n",
       "1  id80133       happy\n",
       "2  id80134       happy\n",
       "3  id80135   not_happy\n",
       "4  id80136       happy"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.columns = ['User_ID', 'Is_Response']\n",
    "submission['Is_Response'] = submission['Is_Response'].apply(lambda x: 'happy' if x == 1 else 'not_happy')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission-joint-learning-4-convs-1-7L.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Convolutional Model With 6 Convolution Layers and 300-d Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 6000\n",
    "MAXLEN = 500\n",
    "BATCH_SIZE = 350\n",
    "EMBEDDING_DIMS = 400\n",
    "FILTERS_1 = 128\n",
    "FILTERS_2 = 256\n",
    "FILTERS_3 = 512\n",
    "HIDDEN_DIMS = 250\n",
    "EPOCHS = 40\n",
    "KERNEL_SIZE = 3\n",
    "PRETRAINING_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def conv_6_e400(model_weights=None):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_FEATURES, EMBEDDING_DIMS, input_length=MAXLEN))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_3, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_3, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(HIDDEN_DIMS * 2, activation='relu') )\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(HIDDEN_DIMS))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "                  metrics=['accuracy'])\n",
    "    if model_weights is not None:\n",
    "        model.load_weights(model_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 500, 400)          2400000   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 500, 400)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 498, 128)          153728    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 249, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 247, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 123, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 121, 256)          98560     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 58, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 29, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 27, 512)           393728    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 13, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 11, 512)           786944    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 5, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 500)               1280500   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 5,485,105\n",
      "Trainable params: 5,485,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = conv_6_e400()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 717619 samples, validate on 7249 samples\n",
      "Epoch 1/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9012Epoch 00001: acc improved from -inf to 0.90121, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-01-0.901.h5py\n",
      "717619/717619 [==============================] - 1768s 2ms/step - loss: 0.2482 - acc: 0.9012 - val_loss: 0.1978 - val_acc: 0.9259\n",
      "Epoch 2/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9267Epoch 00002: acc improved from 0.90121 to 0.92673, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-02-0.927.h5py\n",
      "717619/717619 [==============================] - 1759s 2ms/step - loss: 0.1933 - acc: 0.9267 - val_loss: 0.1879 - val_acc: 0.9290\n",
      "Epoch 3/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9370Epoch 00003: acc improved from 0.92673 to 0.93697, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-03-0.937.h5py\n",
      "717619/717619 [==============================] - 1759s 2ms/step - loss: 0.1679 - acc: 0.9370 - val_loss: 0.1844 - val_acc: 0.9307\n",
      "Epoch 4/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9444Epoch 00004: acc improved from 0.93697 to 0.94437, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-04-0.944.h5py\n",
      "717619/717619 [==============================] - 1758s 2ms/step - loss: 0.1488 - acc: 0.9444 - val_loss: 0.1823 - val_acc: 0.9328\n",
      "Epoch 5/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9494Epoch 00005: acc improved from 0.94437 to 0.94944, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-05-0.949.h5py\n",
      "717619/717619 [==============================] - 1758s 2ms/step - loss: 0.1349 - acc: 0.9494 - val_loss: 0.2080 - val_acc: 0.9290\n",
      "Epoch 6/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9530Epoch 00006: acc improved from 0.94944 to 0.95300, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-06-0.953.h5py\n",
      "717619/717619 [==============================] - 1759s 2ms/step - loss: 0.1250 - acc: 0.9530 - val_loss: 0.1919 - val_acc: 0.9316\n",
      "Epoch 7/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9551Epoch 00007: acc improved from 0.95300 to 0.95510, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-07-0.955.h5py\n",
      "717619/717619 [==============================] - 1758s 2ms/step - loss: 0.1181 - acc: 0.9551 - val_loss: 0.2021 - val_acc: 0.9356\n",
      "Epoch 8/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9570Epoch 00008: acc improved from 0.95510 to 0.95697, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-08-0.957.h5py\n",
      "717619/717619 [==============================] - 1760s 2ms/step - loss: 0.1130 - acc: 0.9570 - val_loss: 0.2044 - val_acc: 0.9342\n",
      "Epoch 9/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9579Epoch 00009: acc improved from 0.95697 to 0.95785, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-09-0.958.h5py\n",
      "717619/717619 [==============================] - 1762s 2ms/step - loss: 0.1101 - acc: 0.9579 - val_loss: 0.2176 - val_acc: 0.9361\n",
      "Epoch 10/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9593Epoch 00010: acc improved from 0.95785 to 0.95929, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-10-0.959.h5py\n",
      "717619/717619 [==============================] - 1763s 2ms/step - loss: 0.1065 - acc: 0.9593 - val_loss: 0.2260 - val_acc: 0.9352\n",
      "Epoch 11/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9598Epoch 00011: acc improved from 0.95929 to 0.95975, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-11-0.960.h5py\n",
      "717619/717619 [==============================] - 1763s 2ms/step - loss: 0.1049 - acc: 0.9598 - val_loss: 0.2289 - val_acc: 0.9360\n",
      "Epoch 12/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9603Epoch 00012: acc improved from 0.95975 to 0.96026, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-12-0.960.h5py\n",
      "717619/717619 [==============================] - 1760s 2ms/step - loss: 0.1029 - acc: 0.9603 - val_loss: 0.2374 - val_acc: 0.9372\n",
      "Epoch 13/40\n",
      "717500/717619 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9610Epoch 00013: acc improved from 0.96026 to 0.96103, saving model to data-aug-joint-3/data-aug-6-convs-tripadvisor-13-0.961.h5py\n",
      "717619/717619 [==============================] - 1760s 2ms/step - loss: 0.1012 - acc: 0.9610 - val_loss: 0.2271 - val_acc: 0.9341\n",
      "Epoch 14/40\n",
      " 36050/717619 [>.............................] - ETA: 27:53 - loss: 0.0970 - acc: 0.9621"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-f9d6c771e03a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(x_train, y_train, validation_data=(x_validate, y_validate), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-3/data-aug-6-convs-tripadvisor-{epoch:02d}-{acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='acc', save_best_only=True)\n",
    "model.fit(x_train, y_train, validation_data=(x_validate, y_validate), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/40\n",
      "35000/35038 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.8510Epoch 00001: val_acc improved from -inf to 0.86364, saving model to data-aug-joint-3/finetuned-2-01-0.864.h5py\n",
      "35038/35038 [==============================] - 94s 3ms/step - loss: 0.3881 - acc: 0.8511 - val_loss: 0.3445 - val_acc: 0.8636\n",
      "Epoch 2/40\n",
      "35000/35038 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.8750Epoch 00002: val_acc improved from 0.86364 to 0.86389, saving model to data-aug-joint-3/finetuned-2-02-0.864.h5py\n",
      "35038/35038 [==============================] - 91s 3ms/step - loss: 0.3149 - acc: 0.8750 - val_loss: 0.3433 - val_acc: 0.8639\n",
      "Epoch 3/40\n",
      "35000/35038 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.8935Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 90s 3ms/step - loss: 0.2736 - acc: 0.8935 - val_loss: 0.3600 - val_acc: 0.8611\n",
      "Epoch 4/40\n",
      "35000/35038 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9152Epoch 00004: val_acc did not improve\n",
      "35038/35038 [==============================] - 90s 3ms/step - loss: 0.2249 - acc: 0.9152 - val_loss: 0.3992 - val_acc: 0.8598\n",
      "Epoch 5/40\n",
      " 3500/35038 [=>............................] - ETA: 1:17 - loss: 0.1790 - acc: 0.9366"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-fc5ed77e58f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-3/finetuned-2-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Finetuning the 4 Conv Layer Model (Last Saved) by Freezing the Embedding Layer (Finetuning #3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = conv_4_e300('data-aug-joint-3/data-aug-4-convs-tripadvisor-06-interrupted.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 500, 300)          1800000   \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 498, 250)          225250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 249, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 247, 250)          187750    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 123, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 121, 500)          375500    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 60, 500)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 58, 500)           750500    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 29, 500)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 14500)             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 250)               3625250   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 7,027,251\n",
      "Trainable params: 7,027,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9007Epoch 00001: val_acc improved from -inf to 0.90447, saving model to data-aug-joint-3/finetuned-3-01-0.904.h5py\n",
      "35038/35038 [==============================] - 102s 3ms/step - loss: 0.2675 - acc: 0.9007 - val_loss: 0.2400 - val_acc: 0.9045\n",
      "Epoch 2/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9105Epoch 00002: val_acc improved from 0.90447 to 0.90652, saving model to data-aug-joint-3/finetuned-3-02-0.907.h5py\n",
      "35038/35038 [==============================] - 102s 3ms/step - loss: 0.2317 - acc: 0.9106 - val_loss: 0.2398 - val_acc: 0.9065\n",
      "Epoch 3/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9195Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 101s 3ms/step - loss: 0.2098 - acc: 0.9195 - val_loss: 0.2466 - val_acc: 0.9045\n",
      "Epoch 4/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9280Epoch 00004: val_acc did not improve\n",
      "35038/35038 [==============================] - 101s 3ms/step - loss: 0.1832 - acc: 0.9280 - val_loss: 0.2623 - val_acc: 0.9014\n",
      "Epoch 5/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9420Epoch 00005: val_acc did not improve\n",
      "35038/35038 [==============================] - 100s 3ms/step - loss: 0.1502 - acc: 0.9419 - val_loss: 0.2853 - val_acc: 0.8991\n",
      "Epoch 6/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9599Epoch 00006: val_acc did not improve\n",
      "35038/35038 [==============================] - 100s 3ms/step - loss: 0.1101 - acc: 0.9598 - val_loss: 0.3367 - val_acc: 0.8986\n",
      "Epoch 7/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9732Epoch 00007: val_acc did not improve\n",
      "35038/35038 [==============================] - 100s 3ms/step - loss: 0.0740 - acc: 0.9732 - val_loss: 0.4242 - val_acc: 0.8929\n",
      "Epoch 8/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9816Epoch 00008: val_acc did not improve\n",
      "35038/35038 [==============================] - 100s 3ms/step - loss: 0.0512 - acc: 0.9814 - val_loss: 0.4770 - val_acc: 0.8924\n",
      "Epoch 9/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9848Epoch 00009: val_acc did not improve\n",
      "35038/35038 [==============================] - 100s 3ms/step - loss: 0.0421 - acc: 0.9847 - val_loss: 0.5258 - val_acc: 0.8783\n",
      "Epoch 10/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9889Epoch 00010: val_acc did not improve\n",
      "35038/35038 [==============================] - 100s 3ms/step - loss: 0.0303 - acc: 0.9888 - val_loss: 0.6195 - val_acc: 0.8919\n",
      "Epoch 11/40\n",
      "15360/35038 [============>.................] - ETA: 54s - loss: 0.0236 - acc: 0.9917"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-21d73bae367b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-3/finetuned-3-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = conv_4_e300('data-aug-joint-3/finetuned-3-02-0.907.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29404/29404 [==============================] - 31s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_y = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission = pd.concat([test['User_ID'], pd.Series(test_y.flatten())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Is_Response\n",
       "0  id80132   not_happy\n",
       "1  id80133       happy\n",
       "2  id80134       happy\n",
       "3  id80135   not_happy\n",
       "4  id80136       happy"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.columns = ['User_ID', 'Is_Response']\n",
    "submission['Is_Response'] = submission['Is_Response'].apply(lambda x: 'happy' if x == 1 else 'not_happy')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission-joint-learning-4-convs-2-7L.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Finetuning the Best 4 Conv Layer Model (Finetuning #4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = conv_4_e300('data-aug-joint-3/data-aug-4-convs-tripadvisor-05-0.982.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9019Epoch 00001: val_acc improved from -inf to 0.90550, saving model to data-aug-joint-3/finetuned-4-01-0.905.h5py\n",
      "35038/35038 [==============================] - 120s 3ms/step - loss: 0.2610 - acc: 0.9021 - val_loss: 0.2379 - val_acc: 0.9055\n",
      "Epoch 2/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9224Epoch 00002: val_acc did not improve\n",
      "35038/35038 [==============================] - 119s 3ms/step - loss: 0.2019 - acc: 0.9226 - val_loss: 0.2454 - val_acc: 0.9050\n",
      "Epoch 3/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9453Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 119s 3ms/step - loss: 0.1494 - acc: 0.9452 - val_loss: 0.2803 - val_acc: 0.8988\n",
      "Epoch 4/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9690Epoch 00004: val_acc did not improve\n",
      "35038/35038 [==============================] - 119s 3ms/step - loss: 0.0906 - acc: 0.9689 - val_loss: 0.3426 - val_acc: 0.8957\n",
      "Epoch 5/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9839Epoch 00005: val_acc did not improve\n",
      "35038/35038 [==============================] - 119s 3ms/step - loss: 0.0460 - acc: 0.9838 - val_loss: 0.4885 - val_acc: 0.8837\n",
      "Epoch 6/40\n",
      " 7424/35038 [=====>........................] - ETA: 1:30 - loss: 0.0191 - acc: 0.9937"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-045dc9e186ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-3/finetuned-4-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29404/29404 [==============================] - 30s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "model = conv_4_e300('data-aug-joint-3/finetuned-4-01-0.905.h5py')\n",
    "test_y = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID  0\n",
       "0  id80132  0\n",
       "1  id80133  1\n",
       "2  id80134  1\n",
       "3  id80135  0\n",
       "4  id80136  1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.concat([test['User_ID'], pd.Series(test_y.flatten())], axis=1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Is_Response\n",
       "0  id80132   not_happy\n",
       "1  id80133       happy\n",
       "2  id80134       happy\n",
       "3  id80135   not_happy\n",
       "4  id80136       happy"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.columns = ['User_ID', 'Is_Response']\n",
    "submission['Is_Response'] = submission['Is_Response'].apply(lambda x: 'happy' if x == 1 else 'not_happy')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission-joint-learning-4-convs-3-7L.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Convolutional Model With 4 Convolution Layers and 300-d Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 6000\n",
    "MAXLEN = 500\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_DIMS = 300\n",
    "FILTERS_1 = 256\n",
    "FILTERS_2 = 512\n",
    "HIDDEN_DIMS = 256\n",
    "EPOCHS = 40\n",
    "KERNEL_SIZE = 3\n",
    "PRETRAINING_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tripadvisor_target = tripadvisor['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_sequences(tripadvisor['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add a batch norm layer after convolution\n",
    "def conv_4_e300_2(model_weights=None):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_FEATURES, EMBEDDING_DIMS, input_length=MAXLEN))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(HIDDEN_DIMS, activation='relu') )\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(HIDDEN_DIMS))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "                  metrics=['accuracy'])\n",
    "    if model_weights is not None:\n",
    "        model.load_weights(model_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = conv_4_e300_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 300)          1800000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 498, 256)          230656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 498, 256)          1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 249, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 247, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 247, 256)          1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 123, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 121, 512)          393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 121, 512)          2048      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 60, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 58, 512)           786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 58, 512)           2048      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 14848)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               3801344   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 7,281,729\n",
      "Trainable params: 7,278,657\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, tripadvisor_target, \n",
    "                                                            random_state=42, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(717619, 500)\n",
      "(717619,)\n",
      "(7249, 500)\n",
      "(7249,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validate.shape)\n",
    "print(y_validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 717619 samples, validate on 7249 samples\n",
      "Epoch 1/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9194Epoch 00001: acc improved from -inf to 0.91939, saving model to data-aug-joint-3/data-aug-4-convs-batchnorm-tripadvisor-01-0.919.h5py\n",
      "717619/717619 [==============================] - 3021s 4ms/step - loss: 0.2032 - acc: 0.9194 - val_loss: 0.1510 - val_acc: 0.9434\n",
      "Epoch 2/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9618Epoch 00002: acc improved from 0.91939 to 0.96183, saving model to data-aug-joint-3/data-aug-4-convs-batchnorm-tripadvisor-02-0.962.h5py\n",
      "717619/717619 [==============================] - 3020s 4ms/step - loss: 0.1082 - acc: 0.9618 - val_loss: 0.1295 - val_acc: 0.9414\n",
      "Epoch 3/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9700Epoch 00003: acc improved from 0.96183 to 0.96996, saving model to data-aug-joint-3/data-aug-4-convs-batchnorm-tripadvisor-03-0.970.h5py\n",
      "717619/717619 [==============================] - 2985s 4ms/step - loss: 0.0845 - acc: 0.9700 - val_loss: 0.0972 - val_acc: 0.9657\n",
      "Epoch 4/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9763Epoch 00004: acc improved from 0.96996 to 0.97633, saving model to data-aug-joint-3/data-aug-4-convs-batchnorm-tripadvisor-04-0.976.h5py\n",
      "717619/717619 [==============================] - 2969s 4ms/step - loss: 0.0669 - acc: 0.9763 - val_loss: 0.1924 - val_acc: 0.9101\n",
      "Epoch 5/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9807Epoch 00005: acc improved from 0.97633 to 0.98073, saving model to data-aug-joint-3/data-aug-4-convs-batchnorm-tripadvisor-05-0.981.h5py\n",
      "717619/717619 [==============================] - 2988s 4ms/step - loss: 0.0538 - acc: 0.9807 - val_loss: 0.2889 - val_acc: 0.8996\n",
      "Epoch 6/40\n",
      "  6656/717619 [..............................] - ETA: 49:27 - loss: 0.0411 - acc: 0.9865"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f0946d26b703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(x_train, y_train, validation_data=(x_validate, y_validate), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-3/data-aug-4-convs-batchnorm-tripadvisor-{epoch:02d}-{acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='acc', save_best_only=True)\n",
    "model.fit(x_train, y_train, validation_data=(x_validate, y_validate), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save('data-aug-joint-3/data-aug-4-convs-batchnorm-tripadvisor-06-interrupted-0.981.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Finetune the ConvNet with BatchNorm without freezing any layer (#6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train['Description'] = train['Description'].apply(clean_text)\n",
    "train['Is_Response'] = train['Is_Response'].apply(lambda x: 1 if x == 'happy' else 0)\n",
    "train_bow = tokenizer.texts_to_sequences(train['Description'])\n",
    "train_bow = sequence.pad_sequences(train_bow, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35038, 500)\n",
      "(3894,)\n",
      "(3894, 500)\n",
      "(3894,)\n"
     ]
    }
   ],
   "source": [
    "train_y = train['Is_Response']\n",
    "train_hx, val_hx, train_hy, val_hy = train_test_split(train_bow, train_y, \n",
    "                                                      random_state=42, test_size=0.1)\n",
    "print(train_hx.shape)\n",
    "print(val_hy.shape)\n",
    "print(val_hx.shape)\n",
    "print(val_hy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9024Epoch 00001: val_acc improved from -inf to 0.90421, saving model to data-aug-joint-3/finetuned-6-01-0.904.h5py\n",
      "35038/35038 [==============================] - 153s 4ms/step - loss: 0.2687 - acc: 0.9023 - val_loss: 0.2973 - val_acc: 0.9042\n",
      "Epoch 2/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9264Epoch 00002: val_acc did not improve\n",
      "35038/35038 [==============================] - 151s 4ms/step - loss: 0.1886 - acc: 0.9264 - val_loss: 0.3651 - val_acc: 0.8130\n",
      "Epoch 3/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9559Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 151s 4ms/step - loss: 0.1135 - acc: 0.9560 - val_loss: 0.3475 - val_acc: 0.8544\n",
      "Epoch 4/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9790Epoch 00004: val_acc did not improve\n",
      "35038/35038 [==============================] - 151s 4ms/step - loss: 0.0561 - acc: 0.9790 - val_loss: 0.4513 - val_acc: 0.8552\n",
      "Epoch 5/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9867Epoch 00005: val_acc did not improve\n",
      "35038/35038 [==============================] - 152s 4ms/step - loss: 0.0379 - acc: 0.9867 - val_loss: 0.3617 - val_acc: 0.8947\n",
      "Epoch 6/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9910Epoch 00006: val_acc did not improve\n",
      "35038/35038 [==============================] - 152s 4ms/step - loss: 0.0269 - acc: 0.9910 - val_loss: 0.3988 - val_acc: 0.8909\n",
      "Epoch 7/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9931Epoch 00007: val_acc did not improve\n",
      "35038/35038 [==============================] - 151s 4ms/step - loss: 0.0201 - acc: 0.9931 - val_loss: 0.4647 - val_acc: 0.8814\n",
      "Epoch 8/40\n",
      " 2304/35038 [>.............................] - ETA: 2:17 - loss: 0.0147 - acc: 0.9957"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2db35e4efe09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-3/finetuned-6-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 717619 samples, validate on 7249 samples\n",
      "Epoch 1/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9809Epoch 00001: acc improved from -inf to 0.98094, saving model to data-aug-joint-3/data-aug-4-convs-batchnorm-II-tripadvisor-01-0.981.h5py\n",
      "717619/717619 [==============================] - 2987s 4ms/step - loss: 0.0526 - acc: 0.9809 - val_loss: 0.1261 - val_acc: 0.9469\n",
      "Epoch 2/40\n",
      "  1024/717619 [..............................] - ETA: 52:09 - loss: 0.0454 - acc: 0.9795"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9d60863b441e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(x_train, y_train, validation_data=(x_validate, y_validate), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-3/data-aug-4-convs-batchnorm-II-tripadvisor-{epoch:02d}-{acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='acc', save_best_only=True)\n",
    "model.fit(x_train, y_train, validation_data=(x_validate, y_validate), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9551Epoch 00001: val_acc improved from -inf to 0.87699, saving model to data-aug-joint-3/finetuned-7-01-0.877.h5py\n",
      "35038/35038 [==============================] - 151s 4ms/step - loss: 0.1218 - acc: 0.9552 - val_loss: 0.5463 - val_acc: 0.8770\n",
      "Epoch 2/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9878Epoch 00002: val_acc did not improve\n",
      "35038/35038 [==============================] - 150s 4ms/step - loss: 0.0359 - acc: 0.9878 - val_loss: 0.7343 - val_acc: 0.8356\n",
      "Epoch 3/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9922Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 150s 4ms/step - loss: 0.0228 - acc: 0.9920 - val_loss: 0.6368 - val_acc: 0.8742\n",
      "Epoch 4/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9944Epoch 00004: val_acc did not improve\n",
      "35038/35038 [==============================] - 149s 4ms/step - loss: 0.0167 - acc: 0.9944 - val_loss: 1.0751 - val_acc: 0.8354\n",
      "Epoch 5/40\n",
      " 1024/35038 [..............................] - ETA: 2:21 - loss: 0.0143 - acc: 0.9951"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-835728c187f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n\u001b[0;32m----> 6\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now that we have done an iteration on supplementary data, \n",
    "# make one more pass on the training data\n",
    "fpath = 'data-aug-joint-3/finetuned-7-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy, validation_data=(val_hx, val_hy), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = conv_4_e300_2('data-aug-joint-3/finetuned-6-01-0.904.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test['Description'] = test['Description'].apply(clean_text)\n",
    "test_x = tokenizer.texts_to_sequences(test['Description'])\n",
    "test_x = sequence.pad_sequences(test_x, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29404/29404 [==============================] - 41s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_y = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Is_Response\n",
       "0  id80132   not_happy\n",
       "1  id80133       happy\n",
       "2  id80134       happy\n",
       "3  id80135   not_happy\n",
       "4  id80136       happy"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.concat([test['User_ID'], pd.Series(test_y.flatten())], axis=1)\n",
    "submission.columns = ['User_ID', 'Is_Response']\n",
    "submission['Is_Response'] = submission['Is_Response'].apply(lambda x: 'happy' if x == 1 else 'not_happy')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission-joint-learning-4-convs-4-7L.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4 Convolutions without BatchNorm. One Hot Encoded Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 6000\n",
    "MAXLEN = 500\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_DIMS = 300\n",
    "FILTERS_1 = 250\n",
    "FILTERS_2 = 500\n",
    "HIDDEN_DIMS = 250\n",
    "EPOCHS = 40\n",
    "KERNEL_SIZE = 3\n",
    "PRETRAINING_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def conv_4_e300_1hot(model_weights=None):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_FEATURES, EMBEDDING_DIMS, input_length=MAXLEN))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_1, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(FILTERS_2, KERNEL_SIZE, padding='valid', strides=1, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(HIDDEN_DIMS, activation='relu') )\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(HIDDEN_DIMS))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=3e-4), \n",
    "                  metrics=['accuracy'])\n",
    "    if model_weights is not None:\n",
    "        model.load_weights(model_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(717619, 2)\n",
      "(7249, 2)\n"
     ]
    }
   ],
   "source": [
    "y_train_1hot = to_categorical(y_train, num_classes=2)\n",
    "y_validate_1hot = to_categorical(y_validate, num_classes=2)\n",
    "print(y_train_1hot.shape)\n",
    "print(y_validate_1hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 300)          1800000   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 498, 250)          225250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 249, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 247, 250)          187750    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 123, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 121, 500)          375500    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 60, 500)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 58, 500)           750500    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 29, 500)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 14500)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 250)               3625250   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 502       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 7,027,502\n",
      "Trainable params: 7,027,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = conv_4_e300_1hot()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 717619 samples, validate on 7249 samples\n",
      "Epoch 1/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9468Epoch 00001: acc improved from -inf to 0.94679, saving model to data-aug-joint-3/data-aug-4-convs-tripadvisor-1hot-01-0.947.h5py\n",
      "717619/717619 [==============================] - 2344s 3ms/step - loss: 0.1386 - acc: 0.9468 - val_loss: 0.1386 - val_acc: 0.9473\n",
      "Epoch 2/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9662Epoch 00002: acc improved from 0.94679 to 0.96620, saving model to data-aug-joint-3/data-aug-4-convs-tripadvisor-1hot-02-0.966.h5py\n",
      "717619/717619 [==============================] - 2341s 3ms/step - loss: 0.0942 - acc: 0.9662 - val_loss: 0.0890 - val_acc: 0.9680\n",
      "Epoch 3/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9731Epoch 00003: acc improved from 0.96620 to 0.97315, saving model to data-aug-joint-3/data-aug-4-convs-tripadvisor-1hot-03-0.973.h5py\n",
      "717619/717619 [==============================] - 2336s 3ms/step - loss: 0.0761 - acc: 0.9731 - val_loss: 0.0887 - val_acc: 0.9670\n",
      "Epoch 4/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9789Epoch 00004: acc improved from 0.97315 to 0.97888, saving model to data-aug-joint-3/data-aug-4-convs-tripadvisor-1hot-04-0.979.h5py\n",
      "717619/717619 [==============================] - 2340s 3ms/step - loss: 0.0602 - acc: 0.9789 - val_loss: 0.0887 - val_acc: 0.9691\n",
      "Epoch 5/40\n",
      "717568/717619 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9830Epoch 00005: acc improved from 0.97888 to 0.98298, saving model to data-aug-joint-3/data-aug-4-convs-tripadvisor-1hot-05-0.983.h5py\n",
      "717619/717619 [==============================] - 2342s 3ms/step - loss: 0.0479 - acc: 0.9830 - val_loss: 0.0981 - val_acc: 0.9702\n",
      "Epoch 6/40\n",
      " 28416/717619 [>.............................] - ETA: 37:16 - loss: 0.0350 - acc: 0.9869"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-d8d3304be176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(x_train, y_train_1hot, validation_data=(x_validate, y_validate_1hot), \n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'data-aug-joint-3/data-aug-4-convs-tripadvisor-1hot-{epoch:02d}-{acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='acc', save_best_only=True)\n",
    "model.fit(x_train, y_train_1hot, validation_data=(x_validate, y_validate_1hot), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9002Epoch 00001: val_acc improved from -inf to 0.90190, saving model to data-aug-joint-3/finetuned-8-01-0.902.h5py\n",
      "35038/35038 [==============================] - 120s 3ms/step - loss: 0.2657 - acc: 0.9001 - val_loss: 0.2449 - val_acc: 0.9019\n",
      "Epoch 2/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9200Epoch 00002: val_acc improved from 0.90190 to 0.90729, saving model to data-aug-joint-3/finetuned-8-02-0.907.h5py\n",
      "35038/35038 [==============================] - 119s 3ms/step - loss: 0.2038 - acc: 0.9200 - val_loss: 0.2425 - val_acc: 0.9073\n",
      "Epoch 3/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9434Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.1509 - acc: 0.9436 - val_loss: 0.2663 - val_acc: 0.9024\n",
      "Epoch 4/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9681Epoch 00004: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.0921 - acc: 0.9681 - val_loss: 0.3510 - val_acc: 0.8939\n",
      "Epoch 5/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9827Epoch 00005: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.0515 - acc: 0.9826 - val_loss: 0.4073 - val_acc: 0.8970\n",
      "Epoch 6/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9901Epoch 00006: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.0294 - acc: 0.9901 - val_loss: 0.5152 - val_acc: 0.8934\n",
      "Epoch 7/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9918Epoch 00007: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.0212 - acc: 0.9918 - val_loss: 0.5524 - val_acc: 0.8888\n",
      "Epoch 8/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9939Epoch 00008: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.0160 - acc: 0.9939 - val_loss: 0.6328 - val_acc: 0.8878\n",
      "Epoch 9/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9932Epoch 00009: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.0173 - acc: 0.9932 - val_loss: 0.6786 - val_acc: 0.8965\n",
      "Epoch 10/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9962Epoch 00010: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.0112 - acc: 0.9961 - val_loss: 0.6798 - val_acc: 0.8919\n",
      "Epoch 11/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9957Epoch 00011: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.0125 - acc: 0.9957 - val_loss: 0.6665 - val_acc: 0.8973\n",
      "Epoch 12/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9962Epoch 00012: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.0104 - acc: 0.9962 - val_loss: 0.7381 - val_acc: 0.8986\n",
      "Epoch 13/40\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9960Epoch 00013: val_acc did not improve\n",
      "35038/35038 [==============================] - 118s 3ms/step - loss: 0.0124 - acc: 0.9960 - val_loss: 0.7268 - val_acc: 0.8957\n",
      "Epoch 14/40\n",
      "24320/35038 [===================>..........] - ETA: 34s - loss: 0.0087 - acc: 0.9972"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-4c0e7614d8f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.fit(train_hx, train_hy1hot, validation_data=(val_hx, val_hy1hot), \n\u001b[0;32m----> 6\u001b[0;31m           callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_hy1hot = to_categorical(val_hy, num_classes=2)\n",
    "train_hy1hot = to_categorical(train_hy, num_classes=2)\n",
    "fpath = 'data-aug-joint-3/finetuned-8-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, monitor='val_acc', save_best_only=True)\n",
    "model.fit(train_hx, train_hy1hot, validation_data=(val_hx, val_hy1hot), \n",
    "          callbacks=[checkpoint], batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = conv_4_e300_1hot('data-aug-joint-3/finetuned-8-02-0.907.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29404/29404 [==============================] - 30s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_y = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID  0\n",
       "0  id80132  0\n",
       "1  id80133  1\n",
       "2  id80134  1\n",
       "3  id80135  0\n",
       "4  id80136  1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.concat([test['User_ID'], pd.Series(test_y.flatten())], axis=1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Is_Response\n",
       "0  id80132   not_happy\n",
       "1  id80133       happy\n",
       "2  id80134       happy\n",
       "3  id80135   not_happy\n",
       "4  id80136       happy"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.columns = ['User_ID', 'Is_Response']\n",
    "submission['Is_Response'] = submission['Is_Response'].apply(lambda x: 'happy' if x == 1 else 'not_happy')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission-joint-learning-4-convs-5-7L.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Use other features along with text predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = conv_4_e300('data-aug-joint-3/finetuned-1-02-0.906.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train['Description'] = train['Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_bow = tokenizer.texts_to_sequences(train['Description'])\n",
    "train_bow = sequence.pad_sequences(train_bow, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predicted_y = model.predict(train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train['Predicted'] = pd.Series(predicted_y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38932/38932 [==============================] - 40s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "train['Predicted_Class'] = pd.Series(model.predict_classes(train_bow).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Browser_Used</th>\n",
       "      <th>Device_Used</th>\n",
       "      <th>Is_Response</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Browser_Val</th>\n",
       "      <th>Device_Val</th>\n",
       "      <th>Predicted_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id10326</td>\n",
       "      <td>the room was kind of clean but had a very stro...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>not happy</td>\n",
       "      <td>0.099726</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id10327</td>\n",
       "      <td>i stayed at the crown plaza april   april   th...</td>\n",
       "      <td>Internet Explorer</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>not happy</td>\n",
       "      <td>0.103372</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id10328</td>\n",
       "      <td>i booked this hotel through hotwire at the low...</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>not happy</td>\n",
       "      <td>0.007604</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id10329</td>\n",
       "      <td>stayed here with husband and sons on the way t...</td>\n",
       "      <td>InternetExplorer</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>happy</td>\n",
       "      <td>0.999781</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id10330</td>\n",
       "      <td>my girlfriends and i stayed here to celebrate ...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>not happy</td>\n",
       "      <td>0.208506</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID                                        Description  \\\n",
       "0  id10326  the room was kind of clean but had a very stro...   \n",
       "1  id10327  i stayed at the crown plaza april   april   th...   \n",
       "2  id10328  i booked this hotel through hotwire at the low...   \n",
       "3  id10329  stayed here with husband and sons on the way t...   \n",
       "4  id10330  my girlfriends and i stayed here to celebrate ...   \n",
       "\n",
       "        Browser_Used Device_Used Is_Response  Predicted  Browser_Val  \\\n",
       "0               Edge      Mobile   not happy   0.099726            1   \n",
       "1  Internet Explorer      Mobile   not happy   0.103372            5   \n",
       "2            Mozilla      Tablet   not happy   0.007604            7   \n",
       "3   InternetExplorer     Desktop       happy   0.999781            6   \n",
       "4               Edge      Tablet   not happy   0.208506            1   \n",
       "\n",
       "   Device_Val  Predicted_Class  \n",
       "0           1                0  \n",
       "1           1                0  \n",
       "2           2                0  \n",
       "3           0                1  \n",
       "4           2                0  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "browser_labels = LabelEncoder()\n",
    "browser_labels.fit(train['Browser_Used'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "browser_vals = browser_labels.transform(train['Browser_Used'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train['Browser_Val'] = pd.Series(browser_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device_labels = LabelEncoder()\n",
    "device_vals = device_labels.fit_transform(train['Device_Used'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train['Device_Val'] = pd.Series(device_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Browser_Used</th>\n",
       "      <th>Device_Used</th>\n",
       "      <th>Is_Response</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Browser_Val</th>\n",
       "      <th>Device_Val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id10326</td>\n",
       "      <td>the room was kind of clean but had a very stro...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>not happy</td>\n",
       "      <td>0.099726</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id10327</td>\n",
       "      <td>i stayed at the crown plaza april   april   th...</td>\n",
       "      <td>Internet Explorer</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>not happy</td>\n",
       "      <td>0.103372</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id10328</td>\n",
       "      <td>i booked this hotel through hotwire at the low...</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>not happy</td>\n",
       "      <td>0.007604</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id10329</td>\n",
       "      <td>stayed here with husband and sons on the way t...</td>\n",
       "      <td>InternetExplorer</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>happy</td>\n",
       "      <td>0.999781</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id10330</td>\n",
       "      <td>my girlfriends and i stayed here to celebrate ...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>not happy</td>\n",
       "      <td>0.208506</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID                                        Description  \\\n",
       "0  id10326  the room was kind of clean but had a very stro...   \n",
       "1  id10327  i stayed at the crown plaza april   april   th...   \n",
       "2  id10328  i booked this hotel through hotwire at the low...   \n",
       "3  id10329  stayed here with husband and sons on the way t...   \n",
       "4  id10330  my girlfriends and i stayed here to celebrate ...   \n",
       "\n",
       "        Browser_Used Device_Used Is_Response  Predicted  Browser_Val  \\\n",
       "0               Edge      Mobile   not happy   0.099726            1   \n",
       "1  Internet Explorer      Mobile   not happy   0.103372            5   \n",
       "2            Mozilla      Tablet   not happy   0.007604            7   \n",
       "3   InternetExplorer     Desktop       happy   0.999781            6   \n",
       "4               Edge      Tablet   not happy   0.208506            1   \n",
       "\n",
       "   Device_Val  \n",
       "0           1  \n",
       "1           1  \n",
       "2           2  \n",
       "3           0  \n",
       "4           2  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train-modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_x_train = train[['Predicted', 'Browser_Val', 'Device_Val']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_y_train = train['Is_Response'].apply(lambda x: 1 if x == 'happy' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lxtrain, lxval, lytrain, lyval = train_test_split(log_x_train, log_y_train, \n",
    "                                                  random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(lxtrain, lytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(cxtrain, cytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "predy = xgbc.predict(cxval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89445300462249611"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predy, cyval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test['Description'] = test['Description'].apply(clean_text)\n",
    "ltext_x = tokenizer.texts_to_sequences(test['Description'])\n",
    "ltext_x = sequence.pad_sequences(ltext_x, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltext_y = model.predict(ltext_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Predicted'] = pd.Series(ltext_y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Browser_Val'] = pd.Series(pd.Series(browser_labels.transform(test['Browser_Used'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Device_Val'] = pd.Series(pd.Series(device_labels.transform(test['Device_Used'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Browser_Used</th>\n",
       "      <th>Device_Used</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Browser_Val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>looking for a motel in close proximity to tv t...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>walking distance to madison square garden and ...</td>\n",
       "      <td>InternetExplorer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.897730</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>visited seattle on business spent  nights in t...</td>\n",
       "      <td>IE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.589763</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>this hotel location is excellent and the rooms...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>this hotel is awesome i love the service antho...</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997413</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID                                        Description  \\\n",
       "0  id80132  looking for a motel in close proximity to tv t...   \n",
       "1  id80133  walking distance to madison square garden and ...   \n",
       "2  id80134  visited seattle on business spent  nights in t...   \n",
       "3  id80135  this hotel location is excellent and the rooms...   \n",
       "4  id80136  this hotel is awesome i love the service antho...   \n",
       "\n",
       "       Browser_Used  Device_Used  Predicted  Browser_Val  \n",
       "0           Firefox            1   0.003549            2  \n",
       "1  InternetExplorer            0   0.897730            6  \n",
       "2                IE            2   0.589763            4  \n",
       "3              Edge            1   0.011710            1  \n",
       "4           Mozilla            1   0.997413            7  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x_test = test[['Predicted', 'Browser_Val', 'Device_Used']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x_test.columns = ['Predicted', 'Browser_Val', 'Device_Val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgbc.predict(log_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id80132</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id80133</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id80134</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id80135</td>\n",
       "      <td>not_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id80136</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Is_Response\n",
       "0  id80132   not_happy\n",
       "1  id80133       happy\n",
       "2  id80134       happy\n",
       "3  id80135   not_happy\n",
       "4  id80136       happy"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.concat([test['User_ID'], pd.Series(predictions.flatten())], axis=1)\n",
    "submission.columns = ['User_ID', 'Is_Response']\n",
    "submission['Is_Response'] = submission['Is_Response'].apply(lambda x: 'happy' if x == 1 else 'not_happy')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission-xgboost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(lxtrain, lytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89753466872110943"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = logreg.predict(lxval)\n",
    "accuracy_score(ys, lyval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(min(train['Browser_Val']))\n",
    "print(max(train['Browser_Val']))\n",
    "print(min(train['Device_Val']))\n",
    "print(max(train['Device_Val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(lxtrain, label=lytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params = {'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'learning_rate': 0.05, \n",
    "    'max_depth': 7, \n",
    "    'num_leaves': 21, \n",
    "    'feature_fraction': 0.3, \n",
    "    'bagging_fraction': 0.8, \n",
    "    'bagging_freq': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tcv_agg's binary_error: 0.681317 + 3.07796e-05\n",
      "[40]\tcv_agg's binary_error: 0.681317 + 3.07796e-05\n"
     ]
    }
   ],
   "source": [
    "lgb_cv = lgb.cv(params, d_train, num_boost_round=500, nfold= 5, shuffle=True, stratified=True, verbose_eval=20, early_stopping_rounds=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nround = lgb_cv['binary_error-mean'].index(np.min(lgb_cv['binary_error-mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'evaluation_result_list' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-7e5f9c23a261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlgbmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnround\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_training_booster\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'evaluation_result_list' referenced before assignment"
     ]
    }
   ],
   "source": [
    "## train the model\n",
    "lgbmodel = lgb.train(params, d_train, num_boost_round=nround)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_x_train = train[['Predicted_Class', 'Browser_Val', 'Device_Val']]\n",
    "cat_y_train = train['Is_Response'].apply(lambda x: 1 if x == 'happy' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxtrain, cxval, cytrain, cyval = train_test_split(cat_x_train, cat_y_train, \n",
    "                                                  random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predicted_Class    int32\n",
       "Browser_Val        int64\n",
       "Device_Val         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cxtrain.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier,cv, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [0, 1, 2]\n",
    "param = {\n",
    "    'use_best_model':True,\n",
    "    'loss_function':'CrossEntropy',\n",
    "    'eval_metric':'Accuracy',\n",
    "    'iterations':1000,\n",
    "    'depth':6,\n",
    "    'learning_rate':0.03,\n",
    "    'rsm':0.3,\n",
    "    'random_seed':2017,\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dt =  Pool(cxtrain, \n",
    "           label=cytrain,\n",
    "           cat_features=cat_cols,\n",
    "           column_description=None,\n",
    "           delimiter='\\t',\n",
    "           has_header=None,\n",
    "           weight=None, \n",
    "           baseline=None,\n",
    "           feature_names=None,\n",
    "           thread_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/catboost/core.py:1664: UserWarning: Parameter \"use_best_model\" has no effect in cross-validation and is ignored\n",
      "  warnings.warn('Parameter \"use_best_model\" has no effect in cross-validation and is ignored')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0: 0: 0: 0: 1: 1: 1: 1: 1: 2: 2: 2: 2: 2: 3: 3: 3: 3: 3: 4: 4: 4: 4: 4: 5: 5: 5: 5: 5: 6: 6: 6: 6: 6: 7: 7: 7: 7: 7: 8: 8: 8: 8: 8: 9: 9: 9: 9: 9: 10: 10: 10: 10: 10: 11: 11: 11: 11: 11: 12: 12: 12: 12: 12: 13: 13: 13: 13: 13: 14: 14: 14: 14: 14: 15: 15: 15: 15: 15: 16: 16: 16: 16: 16: 17: 17: 17: 17: 17: 18: 18: 18: 18: 18: 19: 19: 19: 19: 19: 20: 20: 20: 20: 20: 21: 21: 21: 21: 21: 22: 22: 22: 22: 22: 23: 23: 23: 23: 23: 24: 24: 24: 24: 24: 25: 25: 25: 25: 25: 26: 26: 26: 26: 26: 27: 27: 27: 27: 27: 28: 28: 28: 28: 28: 29: 29: 29: 29: 29: 30: 30: 30: 30: 30: 31: 31: 31: 31: 31: 32: 32: 32: 32: 32: 33: 33: 33: 33: 33: 34: 34: 34: 34: 34: 35: 35: 35: 35: 35: 36: 36: 36: 36: 36: 37: 37: 37: 37: 37: 38: 38: 38: 38: 38: 39: 39: 39: 39: 39: 40: 40: 40: 40: 40: 41: 41: 41: 41: 41: 42: 42: 42: 42: 42: 43: 43: 43: 43: 43: 44: 44: 44: 44: 44: 45: 45: 45: 45: 45: 46: 46: 46: 46: 46: 47: 47: 47: 47: 47: 48: 48: 48: 48: 48: 49: 49: 49: 49: 49: 50: 50: 50: 50: 50: 51: 51: 51: 51: 51: 52: 52: 52: 52: 52: 53: 53: 53: 53: 53: 54: 54: 54: 54: 54: 55: 55: 55: 55: 55: 56: 56: 56: 56: 56: 57: 57: 57: 57: 57: 58: 58: 58: 58: 58: 59: 59: 59: 59: 59: 60: 60: 60: 60: 60: 61: 61: 61: 61: 61: 62: 62: 62: 62: 62: 63: 63: 63: 63: 63: 64: 64: 64: 64: 64: 65: 65: 65: 65: 65: 66: 66: 66: 66: 66: 67: 67: 67: 67: 67: 68: 68: 68: 68: 68: 69: 69: 69: 69: 69: 70: 70: 70: 70: 70: 71: 71: 71: 71: 71: 72: 72: 72: 72: 72: 73: 73: 73: 73: 73: 74: 74: 74: 74: 74: 75: 75: 75: 75: 75: 76: 76: 76: 76: 76: 77: 77: 77: 77: 77: 78: 78: 78: 78: 78: 79: 79: 79: 79: 79: 80: 80: 80: 80: 80: 81: 81: 81: 81: 81: 82: 82: 82: 82: 82: 83: 83: 83: 83: 83: 84: 84: 84: 84: 84: 85: 85: 85: 85: 85: 86: 86: 86: 86: 86: 87: 87: 87: 87: 87: 88: 88: 88: 88: 88: 89: 89: 89: 89: 89: 90: 90: 90: 90: 90: 91: 91: 91: 91: 91: 92: 92: 92: 92: 92: 93: 93: 93: 93: 93: 94: 94: 94: 94: 94: 95: 95: 95: 95: 95: 96: 96: 96: 96: 96: 97: 97: 97: 97: 97: 98: 98: 98: 98: 98: 99: 99: 99: 99: 99: 100: 100: 100: 100: 100: 101: 101: 101: 101: 101: 102: 102: 102: 102: 102: 103: 103: 103: 103: 103: 104: 104: 104: 104: 104: 105: 105: 105: 105: 105: 106: 106: 106: 106: 106: 107: 107: 107: 107: 107: 108: 108: 108: 108: 108: 109: 109: 109: 109: 109: 110: 110: 110: 110: 110: 111: 111: 111: 111: 111: 112: 112: 112: 112: 112: 113: 113: 113: 113: 113: 114: 114: 114: 114: 114: 115: 115: 115: 115: 115: 116: 116: 116: 116: 116: 117: 117: 117: 117: 117: 118: 118: 118: 118: 118: 119: 119: 119: 119: 119: 120: 120: 120: 120: 120: 121: 121: 121: 121: 121: 122: 122: 122: 122: 122: 123: 123: 123: 123: 123: 124: 124: 124: 124: 124: 125: 125: 125: 125: 125: 126: 126: 126: 126: 126: 127: 127: 127: 127: 127: 128: 128: 128: 128: 128: 129: 129: 129: 129: 129: 130: 130: 130: 130: 130: 131: 131: 131: 131: 131: 132: 132: 132: 132: 132: 133: 133: 133: 133: 133: 134: 134: 134: 134: 134: 135: 135: 135: 135: 135: 136: 136: 136: 136: 136: 137: 137: 137: 137: 137: 138: 138: 138: 138: 138: 139: 139: 139: 139: 139: 140: 140: 140: 140: 140: 141: 141: 141: 141: 141: 142: 142: 142: 142: 142: 143: 143: 143: 143: 143: 144: 144: 144: 144: 144: 145: 145: 145: 145: 145: 146: 146: 146: 146: 146: 147: 147: 147: 147: 147: 148: 148: 148: 148: 148: 149: 149: 149: 149: 149: 150: 150: 150: 150: 150: 151: 151: 151: 151: 151: 152: 152: 152: 152: 152: 153: 153: 153: 153: 153: 154: 154: 154: 154: 154: 155: 155: 155: 155: 155: 156: 156: 156: 156: 156: 157: 157: 157: 157: 157: 158: 158: 158: 158: 158: 159: 159: 159: 159: 159: 160: 160: 160: 160: 160: 161: 161: 161: 161: 161: 162: 162: 162: 162: 162: 163: 163: 163: 163: 163: 164: 164: 164: 164: 164: 165: 165: 165: 165: 165: 166: 166: 166: 166: 166: 167: 167: 167: 167: 167: 168: 168: 168: 168: 168: 169: 169: 169: 169: 169: 170: 170: 170: 170: 170: 171: 171: 171: 171: 171: 172: 172: 172: 172: 172: 173: 173: 173: 173: 173: 174: 174: 174: 174: 174: 175: 175: 175: 175: 175: 176: 176: 176: 176: 176: 177: 177: 177: 177: 177: 178: 178: 178: 178: 178: 179: 179: 179: 179: 179: 180: 180: 180: 180: 180: 181: 181: 181: 181: 181: 182: 182: 182: 182: 182: 183: 183: 183: 183: 183: 184: 184: 184: 184: 184: 185: 185: 185: 185: 185: 186: 186: 186: 186: 186: 187: 187: 187: 187: 187: 188: 188: 188: 188: 188: 189: 189: 189: 189: 189: 190: 190: 190: 190: 190: 191: 191: 191: 191: 191: 192: 192: 192: 192: 192: 193: 193: 193: 193: 193: 194: 194: 194: 194: 194: 195: 195: 195: 195: 195: 196: 196: 196: 196: 196: 197: 197: 197: 197: 197: 198: 198: 198: 198: 198: 199: 199: 199: 199: 199: 200: 200: 200: 200: 200: 201: 201: 201: 201: 201: 202: 202: 202: 202: 202: 203: 203: 203: 203: 203: 204: 204: 204: 204: 204: 205: 205: 205: 205: 205: 206: 206: 206: 206: 206: 207: 207: 207: 207: 207: 208: 208: 208: 208: 208: 209: 209: 209: 209: 209: 210: 210: 210: 210: 210: 211: 211: 211: 211: 211: 212: 212: 212: 212: 212: 213: 213: 213: 213: 213: 214: 214: 214: 214: 214: 215: 215: 215: 215: 215: 216: 216: 216: 216: 216: 217: 217: 217: 217: 217: 218: 218: 218: 218: 218: 219: 219: 219: 219: 219: 220: 220: 220: 220: 220: 221: 221: 221: 221: 221: 222: 222: 222: 222: 222: 223: 223: 223: 223: 223: 224: 224: 224: 224: 224: 225: 225: 225: 225: 225: 226: 226: 226: 226: 226: 227: 227: 227: 227: 227: 228: 228: 228: 228: 228: 229: 229: 229: 229: 229: 230: 230: 230: 230: 230: 231: 231: 231: 231: 231: 232: 232: 232: 232: 232: 233: 233: 233: 233: 233: 234: 234: 234: 234: 234: 235: 235: 235: 235: 235: 236: 236: 236: 236: 236: 237: 237: 237: 237: 237: 238: 238: 238: 238: 238: 239: 239: 239: 239: 239: 240: 240: 240: 240: 240: 241: 241: 241: 241: 241: 242: 242: 242: 242: 242: 243: 243: 243: 243: 243: 244: 244: 244: 244: 244: 245: 245: 245: 245: 245: 246: 246: 246: 246: 246: 247: 247: 247: 247: 247: 248: 248: 248: 248: 248: 249: 249: 249: 249: 249: 250: 250: 250: 250: 250: 251: 251: 251: 251: 251: 252: 252: 252: 252: 252: 253: 253: 253: 253: 253: 254: 254: 254: 254: 254: 255: 255: 255: 255: 255: 256: 256: 256: 256: 256: 257: 257: 257: 257: 257: 258: 258: 258: 258: 258: 259: 259: 259: 259: 259: 260: 260: 260: 260: 260: 261: 261: 261: 261: 261: 262: 262: 262: 262: 262: 263: 263: 263: 263: 263: 264: 264: 264: 264: 264: 265: 265: 265: 265: 265: 266: 266: 266: 266: 266: 267: 267: 267: 267: 267: 268: 268: 268: 268: 268: 269: 269: 269: 269: 269: 270: 270: 270: 270: 270: 271: 271: 271: 271: 271: 272: 272: 272: 272: 272: 273: 273: 273: 273: 273: 274: 274: 274: 274: 274: 275: 275: 275: 275: 275: 276: 276: 276: 276: 276: 277: 277: 277: 277: 277: 278: 278: 278: 278: 278: 279: 279: 279: 279: 279: 280: 280: 280: 280: 280: 281: 281: 281: 281: 281: 282: 282: 282: 282: 282: 283: 283: 283: 283: 283: 284: 284: 284: 284: 284: 285: 285: 285: 285: 285: 286: 286: 286: 286: 286: 287: 287: 287: 287: 287: 288: 288: 288: 288: 288: 289: 289: 289: 289: 289: 290: 290: 290: 290: 290: 291: 291: 291: 291: 291: 292: 292: 292: 292: 292: 293: 293: 293: 293: 293: 294: 294: 294: 294: 294: 295: 295: 295: 295: 295: 296: 296: 296: 296: 296: 297: 297: 297: 297: 297: 298: 298: 298: 298: 298: 299: 299: 299: 299: 299: 300: 300: 300: 300: 300: 301: 301: 301: 301: 301: 302: 302: 302: 302: 302: 303: 303: 303: 303: 303: 304: 304: 304: 304: 304: 305: 305: 305: 305: 305: 306: 306: 306: 306: 306: 307: 307: 307: 307: 307: 308: 308: 308: 308: 308: 309: 309: 309: 309: 309: 310: 310: 310: 310: 310: 311: 311: 311: 311: 311: 312: 312: 312: 312: 312: 313: 313: 313: 313: 313: 314: 314: 314: 314: 314: 315: 315: 315: 315: 315: 316: 316: 316: 316: 316: 317: 317: 317: 317: 317: 318: 318: 318: 318: 318: 319: 319: 319: 319: 319: 320: 320: 320: 320: 320: 321: 321: 321: 321: 321: 322: 322: 322: 322: 322: 323: 323: 323: 323: 323: 324: 324: 324: 324: 324: 325: 325: 325: 325: 325: 326: 326: 326: 326: 326: 327: 327: 327: 327: 327: 328: 328: 328: 328: 328: 329: 329: 329: 329: 329: 330: 330: 330: 330: 330: 331: 331: 331: 331: 331: 332: 332: 332: 332: 332: 333: 333: 333: 333: 333: 334: 334: 334: 334: 334: 335: 335: 335: 335: 335: 336: 336: 336: 336: 336: 337: 337: 337: 337: 337: 338: 338: 338: 338: 338: 339: 339: 339: 339: 339: 340: 340: 340: 340: 340: 341: 341: 341: 341: 341: 342: 342: 342: 342: 342: 343: 343: 343: 343: 343: 344: 344: 344: 344: 344: 345: 345: 345: 345: 345: 346: 346: 346: 346: 346: 347: 347: 347: 347: 347: 348: 348: 348: 348: 348: 349: 349: 349: 349: 349: 350: 350: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350: 350: 350: 351: 351: 351: 351: 351: 352: 352: 352: 352: 352: 353: 353: 353: 353: 353: 354: 354: 354: 354: 354: 355: 355: 355: 355: 355: 356: 356: 356: 356: 356: 357: 357: 357: 357: 357: 358: 358: 358: 358: 358: 359: 359: 359: 359: 359: 360: 360: 360: 360: 360: 361: 361: 361: 361: 361: 362: 362: 362: 362: 362: 363: 363: 363: 363: 363: 364: 364: 364: 364: 364: 365: 365: 365: 365: 365: 366: 366: 366: 366: 366: 367: 367: 367: 367: 367: 368: 368: 368: 368: 368: 369: 369: 369: 369: 369: 370: 370: 370: 370: 370: 371: 371: 371: 371: 371: 372: 372: 372: 372: 372: 373: 373: 373: 373: 373: 374: 374: 374: 374: 374: 375: 375: 375: 375: 375: 376: 376: 376: 376: 376: 377: 377: 377: 377: 377: 378: 378: 378: 378: 378: 379: 379: 379: 379: 379: 380: 380: 380: 380: 380: 381: 381: 381: 381: 381: 382: 382: 382: 382: 382: 383: 383: 383: 383: 383: 384: 384: 384: 384: 384: 385: 385: 385: 385: 385: 386: 386: 386: 386: 386: 387: 387: 387: 387: 387: 388: 388: 388: 388: 388: 389: 389: 389: 389: 389: 390: 390: 390: 390: 390: 391: 391: 391: 391: 391: 392: 392: 392: 392: 392: 393: 393: 393: 393: 393: 394: 394: 394: 394: 394: 395: 395: 395: 395: 395: 396: 396: 396: 396: 396: 397: 397: 397: 397: 397: 398: 398: 398: 398: 398: 399: 399: 399: 399: 399: 400: 400: 400: 400: 400: 401: 401: 401: 401: 401: 402: 402: 402: 402: 402: 403: 403: 403: 403: 403: 404: 404: 404: 404: 404: 405: 405: 405: 405: 405: 406: 406: 406: 406: 406: 407: 407: 407: 407: 407: 408: 408: 408: 408: 408: 409: 409: 409: 409: 409: 410: 410: 410: 410: 410: 411: 411: 411: 411: 411: 412: 412: 412: 412: 412: 413: 413: 413: 413: 413: 414: 414: 414: 414: 414: 415: 415: 415: 415: 415: 416: 416: 416: 416: 416: 417: 417: 417: 417: 417: 418: 418: 418: 418: 418: 419: 419: 419: 419: 419: 420: 420: 420: 420: 420: 421: 421: 421: 421: 421: 422: 422: 422: 422: 422: 423: 423: 423: 423: 423: 424: 424: 424: 424: 424: 425: 425: 425: 425: 425: 426: 426: 426: 426: 426: 427: 427: 427: 427: 427: 428: 428: 428: 428: 428: 429: 429: 429: 429: 429: 430: 430: 430: 430: 430: 431: 431: 431: 431: 431: 432: 432: 432: 432: 432: 433: 433: 433: 433: 433: 434: 434: 434: 434: 434: 435: 435: 435: 435: 435: 436: 436: 436: 436: 436: 437: 437: 437: 437: 437: 438: 438: 438: 438: 438: 439: 439: 439: 439: 439: 440: 440: 440: 440: 440: 441: 441: 441: 441: 441: 442: 442: 442: 442: 442: 443: 443: 443: 443: 443: 444: 444: 444: 444: 444: 445: 445: 445: 445: 445: 446: 446: 446: 446: 446: 447: 447: 447: 447: 447: 448: 448: 448: 448: 448: 449: 449: 449: 449: 449: 450: 450: 450: 450: 450: 451: 451: 451: 451: 451: 452: 452: 452: 452: 452: 453: 453: 453: 453: 453: 454: 454: 454: 454: 454: 455: 455: 455: 455: 455: 456: 456: 456: 456: 456: 457: 457: 457: 457: 457: 458: 458: 458: 458: 458: 459: 459: 459: 459: 459: 460: 460: 460: 460: 460: 461: 461: 461: 461: 461: 462: 462: 462: 462: 462: 463: 463: 463: 463: 463: 464: 464: 464: 464: 464: 465: 465: 465: 465: 465: 466: 466: 466: 466: 466: 467: 467: 467: 467: 467: 468: 468: 468: 468: 468: 469: 469: 469: 469: 469: 470: 470: 470: 470: 470: 471: 471: 471: 471: 471: 472: 472: 472: 472: 472: 473: 473: 473: 473: 473: 474: 474: 474: 474: 474: 475: 475: 475: 475: 475: 476: 476: 476: 476: 476: 477: 477: 477: 477: 477: 478: 478: 478: 478: 478: 479: 479: 479: 479: 479: 480: 480: 480: 480: 480: 481: 481: 481: 481: 481: 482: 482: 482: 482: 482: 483: 483: 483: 483: 483: 484: 484: 484: 484: 484: 485: 485: 485: 485: 485: 486: 486: 486: 486: 486: 487: 487: 487: 487: 487: 488: 488: 488: 488: 488: 489: 489: 489: 489: 489: 490: 490: 490: 490: 490: 491: 491: 491: 491: 491: 492: 492: 492: 492: 492: 493: 493: 493: 493: 493: 494: 494: 494: 494: 494: 495: 495: 495: 495: 495: 496: 496: 496: 496: 496: 497: 497: 497: 497: 497: 498: 498: 498: 498: 498: 499: 499: 499: 499: 499: 500: 500: 500: 500: 500: 501: 501: 501: 501: 501: 502: 502: 502: 502: 502: 503: 503: 503: 503: 503: 504: 504: 504: 504: 504: 505: 505: 505: 505: 505: 506: 506: 506: 506: 506: 507: 507: 507: 507: 507: 508: 508: 508: 508: 508: 509: 509: 509: 509: 509: 510: 510: 510: 510: 510: 511: 511: 511: 511: 511: 512: 512: 512: 512: 512: 513: 513: 513: 513: 513: 514: 514: 514: 514: 514: 515: 515: 515: 515: 515: 516: 516: 516: 516: 516: 517: 517: 517: 517: 517: 518: 518: 518: 518: 518: 519: 519: 519: 519: 519: 520: 520: 520: 520: 520: 521: 521: 521: 521: 521: 522: 522: 522: 522: 522: 523: 523: 523: 523: 523: 524: 524: 524: 524: 524: 525: 525: 525: 525: 525: 526: 526: 526: 526: 526: 527: 527: 527: 527: 527: 528: 528: 528: 528: 528: 529: 529: 529: 529: 529: 530: 530: 530: 530: 530: 531: 531: 531: 531: 531: 532: 532: 532: 532: 532: 533: 533: 533: 533: 533: 534: 534: 534: 534: 534: 535: 535: 535: 535: 535: 536: 536: 536: 536: 536: 537: 537: 537: 537: 537: 538: 538: 538: 538: 538: 539: 539: 539: 539: 539: 540: 540: 540: 540: 540: 541: 541: 541: 541: 541: 542: 542: 542: 542: 542: 543: 543: 543: 543: 543: 544: 544: 544: 544: 544: 545: 545: 545: 545: 545: 546: 546: 546: 546: 546: 547: 547: 547: 547: 547: 548: 548: 548: 548: 548: 549: 549: 549: 549: 549: 550: 550: 550: 550: 550: 551: 551: 551: 551: 551: 552: 552: 552: 552: 552: 553: 553: 553: 553: 553: 554: 554: 554: 554: 554: 555: 555: 555: 555: 555: 556: 556: 556: 556: 556: 557: 557: 557: 557: 557: 558: 558: 558: 558: 558: 559: 559: 559: 559: 559: 560: 560: 560: 560: 560: 561: 561: 561: 561: 561: 562: 562: 562: 562: 562: 563: 563: 563: 563: 563: 564: 564: 564: 564: 564: 565: 565: 565: 565: 565: 566: 566: 566: 566: 566: 567: 567: 567: 567: 567: 568: 568: 568: 568: 568: 569: 569: 569: 569: 569: 570: 570: 570: 570: 570: 571: 571: 571: 571: 571: 572: 572: 572: 572: 572: 573: 573: 573: 573: 573: 574: 574: 574: 574: 574: 575: 575: 575: 575: 575: 576: 576: 576: 576: 576: 577: 577: 577: 577: 577: 578: 578: 578: 578: 578: 579: 579: 579: 579: 579: 580: 580: 580: 580: 580: 581: 581: 581: 581: 581: 582: 582: 582: 582: 582: 583: 583: 583: 583: 583: 584: 584: 584: 584: 584: 585: 585: 585: 585: 585: 586: 586: 586: 586: 586: 587: 587: 587: 587: 587: 588: 588: 588: 588: 588: 589: 589: 589: 589: 589: 590: 590: 590: 590: 590: 591: 591: 591: 591: 591: 592: 592: 592: 592: 592: 593: 593: 593: 593: 593: 594: 594: 594: 594: 594: 595: 595: 595: 595: 595: 596: 596: 596: 596: 596: 597: 597: 597: 597: 597: 598: 598: 598: 598: 598: 599: 599: 599: 599: 599: 600: 600: 600: 600: 600: 601: 601: 601: 601: 601: 602: 602: 602: 602: 602: 603: 603: 603: 603: 603: 604: 604: 604: 604: 604: 605: 605: 605: 605: 605: 606: 606: 606: 606: 606: 607: 607: 607: 607: 607: 608: 608: 608: 608: 608: 609: 609: 609: 609: 609: 610: 610: 610: 610: 610: 611: 611: 611: 611: 611: 612: 612: 612: 612: 612: 613: 613: 613: 613: 613: 614: 614: 614: 614: 614: 615: 615: 615: 615: 615: 616: 616: 616: 616: 616: 617: 617: 617: 617: 617: 618: 618: 618: 618: 618: 619: 619: 619: 619: 619: 620: 620: 620: 620: 620: 621: 621: 621: 621: 621: 622: 622: 622: 622: 622: 623: 623: 623: 623: 623: 624: 624: 624: 624: 624: 625: 625: 625: 625: 625: 626: 626: 626: 626: 626: 627: 627: 627: 627: 627: 628: 628: 628: 628: 628: 629: 629: 629: 629: 629: 630: 630: 630: 630: 630: 631: 631: 631: 631: 631: 632: 632: 632: 632: 632: 633: 633: 633: 633: 633: 634: 634: 634: 634: 634: 635: 635: 635: 635: 635: 636: 636: 636: 636: 636: 637: 637: 637: 637: 637: 638: 638: 638: 638: 638: 639: 639: 639: 639: 639: 640: 640: 640: 640: 640: 641: 641: 641: 641: 641: 642: 642: 642: 642: 642: 643: 643: 643: 643: 643: 644: 644: 644: 644: 644: 645: 645: 645: 645: 645: 646: 646: 646: 646: 646: 647: 647: 647: 647: 647: 648: 648: 648: 648: 648: 649: 649: 649: 649: 649: 650: 650: 650: 650: 650: 651: 651: 651: 651: 651: 652: 652: 652: 652: 652: 653: 653: 653: 653: 653: 654: 654: 654: 654: 654: 655: 655: 655: 655: 655: 656: 656: 656: 656: 656: 657: 657: 657: 657: 657: 658: 658: 658: 658: 658: 659: 659: 659: 659: 659: 660: 660: 660: 660: 660: 661: 661: 661: 661: 661: 662: 662: 662: 662: 662: 663: 663: 663: 663: 663: 664: 664: 664: 664: 664: 665: 665: 665: 665: 665: 666: 666: 666: 666: 666: 667: 667: 667: 667: 667: 668: 668: 668: 668: 668: 669: 669: 669: 669: 669: 670: 670: 670: 670: 670: 671: 671: 671: 671: 671: 672: 672: 672: 672: 672: 673: 673: 673: 673: 673: 674: 674: 674: 674: 674: 675: 675: 675: 675: 675: 676: 676: 676: 676: 676: 677: 677: 677: 677: 677: 678: 678: 678: 678: 678: 679: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679: 679: 679: 679: 680: 680: 680: 680: 680: 681: 681: 681: 681: 681: 682: 682: 682: 682: 682: 683: 683: 683: 683: 683: 684: 684: 684: 684: 684: 685: 685: 685: 685: 685: 686: 686: 686: 686: 686: 687: 687: 687: 687: 687: 688: 688: 688: 688: 688: 689: 689: 689: 689: 689: 690: 690: 690: 690: 690: 691: 691: 691: 691: 691: 692: 692: 692: 692: 692: 693: 693: 693: 693: 693: 694: 694: 694: 694: 694: 695: 695: 695: 695: 695: 696: 696: 696: 696: 696: 697: 697: 697: 697: 697: 698: 698: 698: 698: 698: 699: 699: 699: 699: 699: 700: 700: 700: 700: 700: 701: 701: 701: 701: 701: 702: 702: 702: 702: 702: 703: 703: 703: 703: 703: 704: 704: 704: 704: 704: 705: 705: 705: 705: 705: 706: 706: 706: 706: 706: 707: 707: 707: 707: 707: 708: 708: 708: 708: 708: 709: 709: 709: 709: 709: 710: 710: 710: 710: 710: 711: 711: 711: 711: 711: 712: 712: 712: 712: 712: 713: 713: 713: 713: 713: 714: 714: 714: 714: 714: 715: 715: 715: 715: 715: 716: 716: 716: 716: 716: 717: 717: 717: 717: 717: 718: 718: 718: 718: 718: 719: 719: 719: 719: 719: 720: 720: 720: 720: 720: 721: 721: 721: 721: 721: 722: 722: 722: 722: 722: 723: 723: 723: 723: 723: 724: 724: 724: 724: 724: 725: 725: 725: 725: 725: 726: 726: 726: 726: 726: 727: 727: 727: 727: 727: 728: 728: 728: 728: 728: 729: 729: 729: 729: 729: 730: 730: 730: 730: 730: 731: 731: 731: 731: 731: 732: 732: 732: 732: 732: 733: 733: 733: 733: 733: 734: 734: 734: 734: 734: 735: 735: 735: 735: 735: 736: 736: 736: 736: 736: 737: 737: 737: 737: 737: 738: 738: 738: 738: 738: 739: 739: 739: 739: 739: 740: 740: 740: 740: 740: 741: 741: 741: 741: 741: 742: 742: 742: 742: 742: 743: 743: 743: 743: 743: 744: 744: 744: 744: 744: 745: 745: 745: 745: 745: 746: 746: 746: 746: 746: 747: 747: 747: 747: 747: 748: 748: 748: 748: 748: 749: 749: 749: 749: 749: 750: 750: 750: 750: 750: 751: 751: 751: 751: 751: 752: 752: 752: 752: 752: 753: 753: 753: 753: 753: 754: 754: 754: 754: 754: 755: 755: 755: 755: 755: 756: 756: 756: 756: 756: 757: 757: 757: 757: 757: 758: 758: 758: 758: 758: 759: 759: 759: 759: 759: 760: 760: 760: 760: 760: 761: 761: 761: 761: 761: 762: 762: 762: 762: 762: 763: 763: 763: 763: 763: 764: 764: 764: 764: 764: 765: 765: 765: 765: 765: 766: 766: 766: 766: 766: 767: 767: 767: 767: 767: 768: 768: 768: 768: 768: 769: 769: 769: 769: 769: 770: 770: 770: 770: 770: 771: 771: 771: 771: 771: 772: 772: 772: 772: 772: 773: 773: 773: 773: 773: 774: 774: 774: 774: 774: 775: 775: 775: 775: 775: 776: 776: 776: 776: 776: 777: 777: 777: 777: 777: 778: 778: 778: 778: 778: 779: 779: 779: 779: 779: 780: 780: 780: 780: 780: 781: 781: 781: 781: 781: 782: 782: 782: 782: 782: 783: 783: 783: 783: 783: 784: 784: 784: 784: 784: 785: 785: 785: 785: 785: 786: 786: 786: 786: 786: 787: 787: 787: 787: 787: 788: 788: 788: 788: 788: 789: 789: 789: 789: 789: 790: 790: 790: 790: 790: 791: 791: 791: 791: 791: 792: 792: 792: 792: 792: 793: 793: 793: 793: 793: 794: 794: 794: 794: 794: 795: 795: 795: 795: 795: 796: 796: 796: 796: 796: 797: 797: 797: 797: 797: 798: 798: 798: 798: 798: 799: 799: 799: 799: 799: 800: 800: 800: 800: 800: 801: 801: 801: 801: 801: 802: 802: 802: 802: 802: 803: 803: 803: 803: 803: 804: 804: 804: 804: 804: 805: 805: 805: 805: 805: 806: 806: 806: 806: 806: 807: 807: 807: 807: 807: 808: 808: 808: 808: 808: 809: 809: 809: 809: 809: 810: 810: 810: 810: 810: 811: 811: 811: 811: 811: 812: 812: 812: 812: 812: 813: 813: 813: 813: 813: 814: 814: 814: 814: 814: 815: 815: 815: 815: 815: 816: 816: 816: 816: 816: 817: 817: 817: 817: 817: 818: 818: 818: 818: 818: 819: 819: 819: 819: 819: 820: 820: 820: 820: 820: 821: 821: 821: 821: 821: 822: 822: 822: 822: 822: 823: 823: 823: 823: 823: 824: 824: 824: 824: 824: 825: 825: 825: 825: 825: 826: 826: 826: 826: 826: 827: 827: 827: 827: 827: 828: 828: 828: 828: 828: 829: 829: 829: 829: 829: 830: 830: 830: 830: 830: 831: 831: 831: 831: 831: 832: 832: 832: 832: 832: 833: 833: 833: 833: 833: 834: 834: 834: 834: 834: 835: 835: 835: 835: 835: 836: 836: 836: 836: 836: 837: 837: 837: 837: 837: 838: 838: 838: 838: 838: 839: 839: 839: 839: 839: 840: 840: 840: 840: 840: 841: 841: 841: 841: 841: 842: 842: 842: 842: 842: 843: 843: 843: 843: 843: 844: 844: 844: 844: 844: 845: 845: 845: 845: 845: 846: 846: 846: 846: 846: 847: 847: 847: 847: 847: 848: 848: 848: 848: 848: 849: 849: 849: 849: 849: 850: 850: 850: 850: 850: 851: 851: 851: 851: 851: 852: 852: 852: 852: 852: 853: 853: 853: 853: 853: 854: 854: 854: 854: 854: 855: 855: 855: 855: 855: 856: 856: 856: 856: 856: 857: 857: 857: 857: 857: 858: 858: 858: 858: 858: 859: 859: 859: 859: 859: 860: 860: 860: 860: 860: 861: 861: 861: 861: 861: 862: 862: 862: 862: 862: 863: 863: 863: 863: 863: 864: 864: 864: 864: 864: 865: 865: 865: 865: 865: 866: 866: 866: 866: 866: 867: 867: 867: 867: 867: 868: 868: 868: 868: 868: 869: 869: 869: 869: 869: 870: 870: 870: 870: 870: 871: 871: 871: 871: 871: 872: 872: 872: 872: 872: 873: 873: 873: 873: 873: 874: 874: 874: 874: 874: 875: 875: 875: 875: 875: 876: 876: 876: 876: 876: 877: 877: 877: 877: 877: 878: 878: 878: 878: 878: 879: 879: 879: 879: 879: 880: 880: 880: 880: 880: 881: 881: 881: 881: 881: 882: 882: 882: 882: 882: 883: 883: 883: 883: 883: 884: 884: 884: 884: 884: 885: 885: 885: 885: 885: 886: 886: 886: 886: 886: 887: 887: 887: 887: 887: 888: 888: 888: 888: 888: 889: 889: 889: 889: 889: 890: 890: 890: 890: 890: 891: 891: 891: 891: 891: 892: 892: 892: 892: 892: 893: 893: 893: 893: 893: 894: 894: 894: 894: 894: 895: 895: 895: 895: 895: 896: 896: 896: 896: 896: 897: 897: 897: 897: 897: 898: 898: 898: 898: 898: 899: 899: 899: 899: 899: 900: 900: 900: 900: 900: 901: 901: 901: 901: 901: 902: 902: 902: 902: 902: 903: 903: 903: 903: 903: 904: 904: 904: 904: 904: 905: 905: 905: 905: 905: 906: 906: 906: 906: 906: 907: 907: 907: 907: 907: 908: 908: 908: 908: 908: 909: 909: 909: 909: 909: 910: 910: 910: 910: 910: 911: 911: 911: 911: 911: 912: 912: 912: 912: 912: 913: 913: 913: 913: 913: 914: 914: 914: 914: 914: 915: 915: 915: 915: 915: 916: 916: 916: 916: 916: 917: 917: 917: 917: 917: 918: 918: 918: 918: 918: 919: 919: 919: 919: 919: 920: 920: 920: 920: 920: 921: 921: 921: 921: 921: 922: 922: 922: 922: 922: 923: 923: 923: 923: 923: 924: 924: 924: 924: 924: 925: 925: 925: 925: 925: 926: 926: 926: 926: 926: 927: 927: 927: 927: 927: 928: 928: 928: 928: 928: 929: 929: 929: 929: 929: 930: 930: 930: 930: 930: 931: 931: 931: 931: 931: 932: 932: 932: 932: 932: 933: 933: 933: 933: 933: 934: 934: 934: 934: 934: 935: 935: 935: 935: 935: 936: 936: 936: 936: 936: 937: 937: 937: 937: 937: 938: 938: 938: 938: 938: 939: 939: 939: 939: 939: 940: 940: 940: 940: 940: 941: 941: 941: 941: 941: 942: 942: 942: 942: 942: 943: 943: 943: 943: 943: 944: 944: 944: 944: 944: 945: 945: 945: 945: 945: 946: 946: 946: 946: 946: 947: 947: 947: 947: 947: 948: 948: 948: 948: 948: 949: 949: 949: 949: 949: 950: 950: 950: 950: 950: 951: 951: 951: 951: 951: 952: 952: 952: 952: 952: 953: 953: 953: 953: 953: 954: 954: 954: 954: 954: 955: 955: 955: 955: 955: 956: 956: 956: 956: 956: 957: 957: 957: 957: 957: 958: 958: 958: 958: 958: 959: 959: 959: 959: 959: 960: 960: 960: 960: 960: 961: 961: 961: 961: 961: 962: 962: 962: 962: 962: 963: 963: 963: 963: 963: 964: 964: 964: 964: 964: 965: 965: 965: 965: 965: 966: 966: 966: 966: 966: 967: 967: 967: 967: 967: 968: 968: 968: 968: 968: 969: 969: 969: 969: 969: 970: 970: 970: 970: 970: 971: 971: 971: 971: 971: 972: 972: 972: 972: 972: 973: 973: 973: 973: 973: 974: 974: 974: 974: 974: 975: 975: 975: 975: 975: 976: 976: 976: 976: 976: 977: 977: 977: 977: 977: 978: 978: 978: 978: 978: 979: 979: 979: 979: 979: 980: 980: 980: 980: 980: 981: 981: 981: 981: 981: 982: 982: 982: 982: 982: 983: 983: 983: 983: 983: 984: 984: 984: 984: 984: 985: 985: 985: 985: 985: 986: 986: 986: 986: 986: 987: 987: 987: 987: 987: 988: 988: 988: 988: 988: 989: 989: 989: 989: 989: 990: 990: 990: 990: 990: 991: 991: 991: 991: 991: 992: 992: 992: 992: 992: 993: 993: 993: 993: 993: 994: 994: 994: 994: 994: 995: 995: 995: 995: 995: 996: 996: 996: 996: 996: 997: 997: 997: 997: 997: 998: 998: 998: 998: 998: 999: 999: 999: 999: 999: "
     ]
    }
   ],
   "source": [
    "ctb_cv = cv(param, my_dt, fold_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Accuracy_test_avg': [0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948,\n",
       "              0.9377479663193948],\n",
       "             'Accuracy_test_stddev': [0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698,\n",
       "              0.002074029070881698],\n",
       "             'Accuracy_train_avg': [0.9376761442688453,\n",
       "              0.9376761442688453,\n",
       "              0.9376761442688453,\n",
       "              0.9376761442688453,\n",
       "              0.9376761442688453,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.9376832792265706,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.9376975491420214,\n",
       "              0.937690414184296,\n",
       "              0.9377046840997467,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.9376975491420214,\n",
       "              0.9376975491420214,\n",
       "              0.9376975491420214,\n",
       "              0.9376975491420214,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.9377046840997467,\n",
       "              0.9376975491420214,\n",
       "              0.9376975491420214,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.9376975491420214,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.937690414184296,\n",
       "              0.9376975491420214,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377046840997467,\n",
       "              0.9377046840997467,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377118190574721,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377189540151974,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377260889729229,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377332239306483,\n",
       "              0.9377403588883736,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.9377403588883736,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.937747493846099,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377546288038244,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377617637615497,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377688987192752,\n",
       "              0.9377760336770006,\n",
       "              0.9377760336770006,\n",
       "              0.9377760336770006,\n",
       "              0.9377760336770006,\n",
       "              0.9377760336770006,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377831686347259,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377903035924513,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.9377974385501766,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.9378117084656272,\n",
       "              0.9378117084656272,\n",
       "              0.9378117084656272,\n",
       "              0.9378117084656272,\n",
       "              0.9378117084656272,\n",
       "              0.9378117084656272,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.937804573507902,\n",
       "              0.9378117084656274,\n",
       "              0.9378117084656274,\n",
       "              0.9378117084656274,\n",
       "              0.9378117084656274,\n",
       "              0.9378117084656274,\n",
       "              0.9378117084656274],\n",
       "             'Accuracy_train_stddev': [0.000527335646216466,\n",
       "              0.000527335646216466,\n",
       "              0.000527335646216466,\n",
       "              0.000527335646216466,\n",
       "              0.000527335646216466,\n",
       "              0.0005407996735510318,\n",
       "              0.0005407996735510318,\n",
       "              0.0005184517748088642,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005148800250797288,\n",
       "              0.0005099124219430367,\n",
       "              0.0005079117769673173,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005148800250797288,\n",
       "              0.0005148800250797288,\n",
       "              0.0005148800250797288,\n",
       "              0.0005148800250797288,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005209007812334375,\n",
       "              0.0005148800250797288,\n",
       "              0.0005148800250797288,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005148800250797288,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005161144551063011,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005291426268841848,\n",
       "              0.0005291426268841848,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005291426268841848,\n",
       "              0.0005291426268841848,\n",
       "              0.0005291426268841848,\n",
       "              0.0005291426268841848,\n",
       "              0.0005209007812334375,\n",
       "              0.0005209007812334375,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005101619509141467,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005184517748088642,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005099124219430367,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.0005017351786735653,\n",
       "              0.000493938029719118,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000493938029719118,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.000486539249294898,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.0004795572762700096,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.00048823656029745055,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004972763543705759,\n",
       "              0.0004907066851428381,\n",
       "              0.0004907066851428381,\n",
       "              0.0004907066851428381,\n",
       "              0.0004907066851428381,\n",
       "              0.0004907066851428381,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.000484573426107007,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004722026929549031,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.0004600526463309073,\n",
       "              0.00046418370181081054,\n",
       "              0.00046418370181081054,\n",
       "              0.00046418370181081054,\n",
       "              0.00046949973604541994,\n",
       "              0.00046949973604541994,\n",
       "              0.00046949973604541994,\n",
       "              0.00046949973604541994,\n",
       "              0.00046949973604541994,\n",
       "              0.00046949973604541994,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.00046555257444284365,\n",
       "              0.0004536453029577617,\n",
       "              0.0004536453029577617,\n",
       "              0.0004536453029577617,\n",
       "              0.0004536453029577617,\n",
       "              0.0004536453029577617,\n",
       "              0.0004536453029577617],\n",
       "             \"b'Accuracy'_test_avg\": []})"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_round = ctb_cv['Accuracy_test_avg'].index(np.max(ctb_cv['Accuracy_test_avg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=100, learning_rate=0.03,rsm = 0.3 ,depth=6, eval_metric='Accuracy', random_seed=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: learn: 0.9376962\ttotal: 46.9ms\tremaining: 4.64s\n",
      "1: learn: 0.9376962\ttotal: 82.7ms\tremaining: 4.05s\n",
      "2: learn: 0.9377248\ttotal: 109ms\tremaining: 3.53s\n",
      "3: learn: 0.9376962\ttotal: 141ms\tremaining: 3.39s\n",
      "4: learn: 0.9376962\ttotal: 169ms\tremaining: 3.22s\n",
      "5: learn: 0.9376962\ttotal: 192ms\tremaining: 3.01s\n",
      "6: learn: 0.9376962\ttotal: 209ms\tremaining: 2.78s\n",
      "7: learn: 0.9377248\ttotal: 236ms\tremaining: 2.71s\n",
      "8: learn: 0.9377248\ttotal: 262ms\tremaining: 2.65s\n",
      "9: learn: 0.9377248\ttotal: 292ms\tremaining: 2.63s\n",
      "10: learn: 0.9377248\ttotal: 314ms\tremaining: 2.54s\n",
      "11: learn: 0.9377248\ttotal: 342ms\tremaining: 2.51s\n",
      "12: learn: 0.9377248\ttotal: 360ms\tremaining: 2.41s\n",
      "13: learn: 0.9377248\ttotal: 384ms\tremaining: 2.36s\n",
      "14: learn: 0.9377248\ttotal: 412ms\tremaining: 2.33s\n",
      "15: learn: 0.9377248\ttotal: 445ms\tremaining: 2.34s\n",
      "16: learn: 0.9377248\ttotal: 463ms\tremaining: 2.26s\n",
      "17: learn: 0.9377248\ttotal: 484ms\tremaining: 2.2s\n",
      "18: learn: 0.9377248\ttotal: 504ms\tremaining: 2.15s\n",
      "19: learn: 0.9377248\ttotal: 519ms\tremaining: 2.08s\n",
      "20: learn: 0.9377248\ttotal: 544ms\tremaining: 2.04s\n",
      "21: learn: 0.9377248\ttotal: 561ms\tremaining: 1.99s\n",
      "22: learn: 0.9377248\ttotal: 581ms\tremaining: 1.95s\n",
      "23: learn: 0.9377248\ttotal: 602ms\tremaining: 1.91s\n",
      "24: learn: 0.9377248\ttotal: 639ms\tremaining: 1.92s\n",
      "25: learn: 0.9377248\ttotal: 662ms\tremaining: 1.88s\n",
      "26: learn: 0.9377248\ttotal: 685ms\tremaining: 1.85s\n",
      "27: learn: 0.9377248\ttotal: 708ms\tremaining: 1.82s\n",
      "28: learn: 0.9377248\ttotal: 730ms\tremaining: 1.79s\n",
      "29: learn: 0.9377248\ttotal: 748ms\tremaining: 1.74s\n",
      "30: learn: 0.9377248\ttotal: 763ms\tremaining: 1.7s\n",
      "31: learn: 0.9377248\ttotal: 793ms\tremaining: 1.68s\n",
      "32: learn: 0.9377248\ttotal: 814ms\tremaining: 1.65s\n",
      "33: learn: 0.9377248\ttotal: 830ms\tremaining: 1.61s\n",
      "34: learn: 0.9377248\ttotal: 871ms\tremaining: 1.62s\n",
      "35: learn: 0.9377248\ttotal: 901ms\tremaining: 1.6s\n",
      "36: learn: 0.9377248\ttotal: 922ms\tremaining: 1.57s\n",
      "37: learn: 0.9377248\ttotal: 954ms\tremaining: 1.56s\n",
      "38: learn: 0.9377248\ttotal: 977ms\tremaining: 1.53s\n",
      "39: learn: 0.9377248\ttotal: 1s\tremaining: 1.5s\n",
      "40: learn: 0.9377248\ttotal: 1.02s\tremaining: 1.47s\n",
      "41: learn: 0.9377248\ttotal: 1.04s\tremaining: 1.44s\n",
      "42: learn: 0.9377248\ttotal: 1.06s\tremaining: 1.41s\n",
      "43: learn: 0.9377248\ttotal: 1.08s\tremaining: 1.38s\n",
      "44: learn: 0.9377248\ttotal: 1.11s\tremaining: 1.36s\n",
      "45: learn: 0.9377248\ttotal: 1.14s\tremaining: 1.34s\n",
      "46: learn: 0.9377248\ttotal: 1.16s\tremaining: 1.31s\n",
      "47: learn: 0.9377248\ttotal: 1.21s\tremaining: 1.31s\n",
      "48: learn: 0.9377248\ttotal: 1.23s\tremaining: 1.28s\n",
      "49: learn: 0.9377248\ttotal: 1.25s\tremaining: 1.25s\n",
      "50: learn: 0.9377248\ttotal: 1.29s\tremaining: 1.24s\n",
      "51: learn: 0.9377248\ttotal: 1.32s\tremaining: 1.21s\n",
      "52: learn: 0.9377248\ttotal: 1.35s\tremaining: 1.19s\n",
      "53: learn: 0.9377248\ttotal: 1.39s\tremaining: 1.18s\n",
      "54: learn: 0.9377248\ttotal: 1.42s\tremaining: 1.16s\n",
      "55: learn: 0.9377248\ttotal: 1.45s\tremaining: 1.14s\n",
      "56: learn: 0.9377248\ttotal: 1.48s\tremaining: 1.12s\n",
      "57: learn: 0.9377248\ttotal: 1.52s\tremaining: 1.1s\n",
      "58: learn: 0.9377248\ttotal: 1.54s\tremaining: 1.07s\n",
      "59: learn: 0.9377248\ttotal: 1.58s\tremaining: 1.05s\n",
      "60: learn: 0.9377248\ttotal: 1.59s\tremaining: 1.02s\n",
      "61: learn: 0.9377248\ttotal: 1.61s\tremaining: 989ms\n",
      "62: learn: 0.9377248\ttotal: 1.63s\tremaining: 955ms\n",
      "63: learn: 0.9377248\ttotal: 1.64s\tremaining: 924ms\n",
      "64: learn: 0.9377248\ttotal: 1.66s\tremaining: 891ms\n",
      "65: learn: 0.9377248\ttotal: 1.67s\tremaining: 860ms\n",
      "66: learn: 0.9377248\ttotal: 1.68s\tremaining: 829ms\n",
      "67: learn: 0.9377248\ttotal: 1.72s\tremaining: 812ms\n",
      "68: learn: 0.9377248\ttotal: 1.74s\tremaining: 783ms\n",
      "69: learn: 0.9377248\ttotal: 1.76s\tremaining: 754ms\n",
      "70: learn: 0.9377248\ttotal: 1.79s\tremaining: 731ms\n",
      "71: learn: 0.9377248\ttotal: 1.81s\tremaining: 706ms\n",
      "72: learn: 0.9377248\ttotal: 1.85s\tremaining: 683ms\n",
      "73: learn: 0.9377248\ttotal: 1.88s\tremaining: 660ms\n",
      "74: learn: 0.9377248\ttotal: 1.91s\tremaining: 637ms\n",
      "75: learn: 0.9377248\ttotal: 1.94s\tremaining: 613ms\n",
      "76: learn: 0.9377248\ttotal: 1.97s\tremaining: 589ms\n",
      "77: learn: 0.9377248\ttotal: 2s\tremaining: 565ms\n",
      "78: learn: 0.9377248\ttotal: 2.04s\tremaining: 541ms\n",
      "79: learn: 0.9377248\ttotal: 2.07s\tremaining: 518ms\n",
      "80: learn: 0.9377248\ttotal: 2.11s\tremaining: 495ms\n",
      "81: learn: 0.9377248\ttotal: 2.14s\tremaining: 469ms\n",
      "82: learn: 0.9377248\ttotal: 2.17s\tremaining: 445ms\n",
      "83: learn: 0.9377248\ttotal: 2.2s\tremaining: 418ms\n",
      "84: learn: 0.9377248\ttotal: 2.23s\tremaining: 394ms\n",
      "85: learn: 0.9377248\ttotal: 2.25s\tremaining: 367ms\n",
      "86: learn: 0.9377248\ttotal: 2.28s\tremaining: 341ms\n",
      "87: learn: 0.9377248\ttotal: 2.31s\tremaining: 315ms\n",
      "88: learn: 0.9377248\ttotal: 2.34s\tremaining: 289ms\n",
      "89: learn: 0.9377248\ttotal: 2.37s\tremaining: 264ms\n",
      "90: learn: 0.9377248\ttotal: 2.41s\tremaining: 238ms\n",
      "91: learn: 0.9377248\ttotal: 2.45s\tremaining: 213ms\n",
      "92: learn: 0.9377248\ttotal: 2.48s\tremaining: 187ms\n",
      "93: learn: 0.9377248\ttotal: 2.51s\tremaining: 160ms\n",
      "94: learn: 0.9377248\ttotal: 2.55s\tremaining: 134ms\n",
      "95: learn: 0.9377248\ttotal: 2.57s\tremaining: 107ms\n",
      "96: learn: 0.9377248\ttotal: 2.6s\tremaining: 80.3ms\n",
      "97: learn: 0.9377248\ttotal: 2.62s\tremaining: 53.5ms\n",
      "98: learn: 0.9377248\ttotal: 2.66s\tremaining: 26.9ms\n",
      "99: learn: 0.9377248\ttotal: 2.71s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7fb6b52e7d30>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(my_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(cxval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89445300462249611"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preds, cyval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Entity Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_browser = Sequential()\n",
    "model_browser.add(Embedding(11, 7, input_length=1))\n",
    "model_browser.add(Reshape(target_shape=(7,)))\n",
    "models.append(model_browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_device = Sequential()\n",
    "model_device.add(Embedding(11, 2, input_length=1))\n",
    "model_device.add(Reshape(target_shape=(2,)))\n",
    "models.append(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_text = Sequential()\n",
    "model_text.add(Dense(1, input_dim=1))\n",
    "models.append(model_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_10 (Merge)             (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 137,318\n",
      "Trainable params: 137,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fmodel = Sequential()\n",
    "fmodel.add(Merge(models, mode='concat'))\n",
    "fmodel.add(Dense(512, activation='relu'))\n",
    "fmodel.add(Dense(256, activation='relu'))\n",
    "fmodel.add(Dense(1))\n",
    "fmodel.add(Activation('sigmoid'))\n",
    "fmodel.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "fmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lxtrain_ = lxtrain.as_matrix()\n",
    "lxval_ = lxval.as_matrix()\n",
    "lytrain_ = lytrain.as_matrix()\n",
    "lyval_ = lyval.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lytrain_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lxtrain_ = [lxtrain['Predicted'].as_matrix(), lxtrain['Browser_Val'].as_matrix(), \n",
    "            lxtrain['Device_Val'].as_matrix()]\n",
    "lxval_ = [lxval['Predicted'].as_matrix(), lxval['Browser_Val'].as_matrix(), \n",
    "          lxval['Device_Val'].as_matrix()]\n",
    "lytrain_ = lytrain.as_matrix()\n",
    "lyval_ = lyval.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35038,)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lxtrain_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/100\n",
      "34688/35038 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.7049Epoch 00001: val_acc improved from -inf to 0.71289, saving model to entity-embedding/entity-embedding-6-01-0.713.h5py\n",
      "35038/35038 [==============================] - 3s 75us/step - loss: 0.5527 - acc: 0.7054 - val_loss: 0.5457 - val_acc: 0.7129\n",
      "Epoch 2/100\n",
      "34432/35038 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.7072Epoch 00002: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 71us/step - loss: 0.5507 - acc: 0.7071 - val_loss: 0.5440 - val_acc: 0.7121\n",
      "Epoch 3/100\n",
      "34880/35038 [============================>.] - ETA: 0s - loss: 0.5507 - acc: 0.7060Epoch 00003: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 68us/step - loss: 0.5505 - acc: 0.7062 - val_loss: 0.5429 - val_acc: 0.7129\n",
      "Epoch 4/100\n",
      "35008/35038 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.7068Epoch 00004: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 68us/step - loss: 0.5505 - acc: 0.7068 - val_loss: 0.5426 - val_acc: 0.7129\n",
      "Epoch 5/100\n",
      "34752/35038 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.7074Epoch 00005: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 70us/step - loss: 0.5501 - acc: 0.7072 - val_loss: 0.5485 - val_acc: 0.7129\n",
      "Epoch 6/100\n",
      "34624/35038 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.7075Epoch 00006: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 71us/step - loss: 0.5504 - acc: 0.7080 - val_loss: 0.5430 - val_acc: 0.7129\n",
      "Epoch 7/100\n",
      "34496/35038 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7089Epoch 00007: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 69us/step - loss: 0.5500 - acc: 0.7085 - val_loss: 0.5448 - val_acc: 0.7129\n",
      "Epoch 8/100\n",
      "34880/35038 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7079Epoch 00008: val_acc improved from 0.71289 to 0.71418, saving model to entity-embedding/entity-embedding-6-08-0.714.h5py\n",
      "35038/35038 [==============================] - 2s 71us/step - loss: 0.5499 - acc: 0.7076 - val_loss: 0.5434 - val_acc: 0.7142\n",
      "Epoch 9/100\n",
      "34944/35038 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.7073Epoch 00009: val_acc did not improve\n",
      "35038/35038 [==============================] - 3s 75us/step - loss: 0.5502 - acc: 0.7075 - val_loss: 0.5425 - val_acc: 0.7129\n",
      "Epoch 10/100\n",
      "34752/35038 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7077Epoch 00010: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 71us/step - loss: 0.5496 - acc: 0.7074 - val_loss: 0.5466 - val_acc: 0.7129\n",
      "Epoch 11/100\n",
      "34688/35038 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.7078Epoch 00011: val_acc did not improve\n",
      "35038/35038 [==============================] - 3s 72us/step - loss: 0.5500 - acc: 0.7080 - val_loss: 0.5436 - val_acc: 0.7129\n",
      "Epoch 12/100\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.7083Epoch 00012: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 67us/step - loss: 0.5497 - acc: 0.7084 - val_loss: 0.5424 - val_acc: 0.7129\n",
      "Epoch 13/100\n",
      "34880/35038 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7085Epoch 00013: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 71us/step - loss: 0.5496 - acc: 0.7086 - val_loss: 0.5433 - val_acc: 0.7129\n",
      "Epoch 14/100\n",
      "34752/35038 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7086Epoch 00014: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 71us/step - loss: 0.5496 - acc: 0.7085 - val_loss: 0.5426 - val_acc: 0.7083\n",
      "Epoch 15/100\n",
      "34816/35038 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.7068Epoch 00015: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 70us/step - loss: 0.5498 - acc: 0.7069 - val_loss: 0.5426 - val_acc: 0.7129\n",
      "Epoch 16/100\n",
      "34496/35038 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7084Epoch 00016: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 68us/step - loss: 0.5495 - acc: 0.7085 - val_loss: 0.5426 - val_acc: 0.7129\n",
      "Epoch 17/100\n",
      "34752/35038 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.7082Epoch 00017: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 69us/step - loss: 0.5492 - acc: 0.7081 - val_loss: 0.5446 - val_acc: 0.7129\n",
      "Epoch 18/100\n",
      "34560/35038 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.7076Epoch 00018: val_acc did not improve\n",
      "35038/35038 [==============================] - 3s 72us/step - loss: 0.5495 - acc: 0.7080 - val_loss: 0.5427 - val_acc: 0.7129\n",
      "Epoch 19/100\n",
      "34496/35038 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7082Epoch 00019: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 70us/step - loss: 0.5494 - acc: 0.7083 - val_loss: 0.5432 - val_acc: 0.7129\n",
      "Epoch 20/100\n",
      "34752/35038 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.7080Epoch 00020: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 69us/step - loss: 0.5495 - acc: 0.7081 - val_loss: 0.5431 - val_acc: 0.7129\n",
      "Epoch 21/100\n",
      "34752/35038 [============================>.] - ETA: 0s - loss: 0.5490 - acc: 0.7084Epoch 00021: val_acc did not improve\n",
      "35038/35038 [==============================] - 2s 66us/step - loss: 0.5494 - acc: 0.7081 - val_loss: 0.5445 - val_acc: 0.7129\n",
      "Epoch 22/100\n",
      "34880/35038 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.7087Epoch 00022: val_acc did not improve\n",
      "35038/35038 [==============================] - 3s 73us/step - loss: 0.5494 - acc: 0.7087 - val_loss: 0.5424 - val_acc: 0.7129\n",
      "Epoch 23/100\n",
      "21952/35038 [=================>............] - ETA: 0s - loss: 0.5501 - acc: 0.7068"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-ee4e99acd36e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                              monitor='val_acc', save_best_only=True)\n\u001b[1;32m      4\u001b[0m fmodel.fit(lxtrain_, lytrain_, validation_data=(lxval_, lyval_), \n\u001b[0;32m----> 5\u001b[0;31m           callbacks=[checkpoint], batch_size=64, epochs=100)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fpath = 'entity-embedding/entity-embedding-6-{epoch:02d}-{val_acc:.3f}.h5py'\n",
    "checkpoint = ModelCheckpoint(filepath=fpath, verbose=1, \n",
    "                             monitor='val_acc', save_best_only=True)\n",
    "fmodel.fit(lxtrain_, lytrain_, validation_data=(lxval_, lyval_), \n",
    "          callbacks=[checkpoint], batch_size=64, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
